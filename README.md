# HSQP: A Plug-and-Play Symbolicâ€“Quantized Framework for Time-Series Tokenization in Large Language Models

<p align="center">
  <b>Time-Series Tokenization Â· Symbolic Representation Â· Quantization Â· LLMs</b>
</p>

---

## ğŸ” Overview

**HSQP** is a **plug-and-play symbolicâ€“quantized framework** designed to convert continuous time-series into **compact, semantically meaningful token sequences** suitable for **Large Language Models (LLMs)**.

Unlike conventional discretization or neural-only tokenizers, HSQP integrates:

- **Symbolic aggregation** for temporal structure preservation  
- **Quantization** for numerical compactness  
- **Hierarchical tokenization** for scalable long-horizon forecasting  

HSQP can be seamlessly attached to existing forecasting architectures (e.g., PatchTST, TimeLLM) and LLM-based pipelines without retraining the backbone model.

---

## âœ¨ Key Contributions

- A unified **symbolicâ€“quantized tokenization pipeline** for time-series  
- Plug-and-play compatibility with **LLMs and transformer forecasters**  
- Strong performance across **forecasting, compression, and reconstruction**  
- Interpretable and low-entropy token sequences  
- Minimal overhead and modular design  

---

## ğŸ§± Repository Structure

```text
Time-series-Tokenization-For-LLMs/
â”‚
â”œâ”€â”€ data/                    # Datasets (not tracked)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ src/                     # Core reusable modules
â”‚   â”œâ”€â”€ tokenization/        # HSQP, ABBA, quantization
â”‚   â”œâ”€â”€ datasets/            # Dataset loaders
â”‚   â”œâ”€â”€ models/              # Model wrappers
â”‚   â”œâ”€â”€ training/            # Training logic
â”‚   â”œâ”€â”€ evaluation/          # Metrics & evaluation
â”‚   â””â”€â”€ utils/               # Utilities
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/             # YAML experiment configs
â”‚   â””â”€â”€ results/             # Output logs (not tracked)
â”‚
â”œâ”€â”€ demos/                   # Reproducible notebooks
â”‚   â”œâ”€â”€ LLM_ABBA_demo.ipynb
â”‚   â”œâ”€â”€ PatchTST_HSQP_demo.ipynb
â”‚   â”œâ”€â”€ TimeLLM_HSQP_demo.ipynb
â”‚   â””â”€â”€ TimeVQVAE_demo.ipynb
â”‚
â”œâ”€â”€ scripts/                 # Entry-point scripts
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md


## Installations
ğŸ”¹ git clone https://github.com/shamsua/Time-series-Tokenization-For-LLMs.git  
ğŸ”¹ cd Time-series-Tokenization-For-LLMs  
ğŸ”¹ pip install -r requirements.txt  




