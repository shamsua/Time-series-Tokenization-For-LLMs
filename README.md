# HSQP: A Plug-and-Play Symbolic–Quantized Framework for Time-Series Tokenization in Large Language Models
Tokenizingcontinuous time-series signals for Large Language Models (LLMs) remains a fun damental challenge due to the mismatch between numerical temporal data and discrete token-based architectures. Existing approaches address this problem in isolation, often sacrificing numerical fidelity, temporal coherence, or computational efficiency. We propose Hierarchical Symbolic–Quantized Patching (HSQP), a unified tokenization framework that hierarchically integrates patch segmentation, adaptive Brownian Bridge–based symbolic aggregation (ABBA), and affine quantization into a jointly embedded, non-separable token representation. Unlike sequential preprocessing pipelines, HSQP fuses symbolic cluster identities and quantized numerical descriptors into compact dual semantic tokens that simultaneously encode structural temporal patterns and bounded numerical precision. The hierarchical design reduces sequence length, constrains reconstruction drift through patch-level anchoring, and introduces bounded quantization noise that stabilizes downstream Transformer attention. We provide theoretical analysis of quantization error propagation and demonstrate that reconstruction error remains locally bounded under Lipschitz continuity assumptions. Extensive experiments on six benchmark datasets show that HSQPconsistently improves reconstruction fidelity and downstream forecasting accuracy while achieving stable compression ratios and balanced token entropy. Integrated as a plug-and-play module with frozen LLM backbones, HSQP enhances predictive performance without architectural modification. These results establish HSQP as an efficient, interpretable, and scalable tokenization paradigm for adapting continuous time-series data to LLM-based forecasting frameworks.
