{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST + HSQP Implementation for Time Series Forecasting\n",
    "\n",
    "This notebook integrates the Hierarchical Symbolic-Quantized Patching (HSQP) method as a plugin into the original **PatchTST** framework. It is designed to run experiments on one dataset at a time to mitigate the 'session crashed after using all available RAM' issue.\n",
    "\n",
    "**Datasets to be used:** `electricity`, `ETTh1`, `ETTh2`, `ETTm1`, `ETTm2`, `weather`, `traffic`, and `national_illness`.\n",
    "\n",
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch numpy pandas scikit-learn\n",
    "!pip install --upgrade scikit-learn\n",
    "\n",
    "# Clone the PatchTST repository\n",
    "!git clone https://github.com/yuqinie98/PatchTST.git\n",
    "%cd PatchTST/PatchTST_supervised\n",
    "\n",
    "# Set up the environment path\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "sys.path.append('./models')\n",
    "sys.path.append('./layers')\n",
    "sys.path.append('./exp')\n",
    "\n",
    "print(\"Setup complete. PatchTST repository cloned and paths configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HSQP Plugin Implementation\n",
    "\n",
    "The HSQP implementation is provided below. This code will be saved as `models/hsqp_plugin.py` within the cloned repository structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../models/hsqp_plugin.py\n",
    "\"\"\"\n",
    "Hierarchical Symbolic-Quantized Patching (HSQP) Implementation as a Plugin.\n",
    "\n",
    "This module contains the necessary classes for the HSQP method, designed to be\n",
    "integrated into existing time series models like PatchTST.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "# --- 1. TimeSeriesPatching ---\n",
    "class TimeSeriesPatching:\n",
    "    \"\"\"\n",
    "    Class for creating patches from time series data (Step 2 in HSQP).\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_length: int = 24, stride: int = 12, overlap: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the patching parameters.\n",
    "        \n",
    "        Args:\n",
    "            patch_length: Length of each patch\n",
    "            stride: Step size between patches (if overlap=True)\n",
    "            overlap: Whether patches should overlap\n",
    "        \"\"\"\n",
    "        self.patch_length = patch_length\n",
    "        self.stride = stride if overlap else patch_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "    def create_patches(self, time_series: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create patches from a time series.\n",
    "        \n",
    "        Args:\n",
    "            time_series: Time series data of shape [batch_size, seq_length, features]\n",
    "                         or [seq_length, features] or [seq_length]\n",
    "            \n",
    "        Returns:\n",
    "            Patches of shape [batch_size, num_patches, patch_length, features]\n",
    "                         or [num_patches, patch_length, features]\n",
    "                         or [num_patches, patch_length]\n",
    "        \"\"\"\n",
    "        # Handle different input shapes\n",
    "        original_shape = time_series.shape\n",
    "        if len(original_shape) == 1:\n",
    "            # Convert [seq_length] to [seq_length, 1]\n",
    "            time_series = time_series.reshape(-1, 1)\n",
    "            seq_length, features = time_series.shape\n",
    "            batch_size = None\n",
    "        elif len(original_shape) == 2:\n",
    "            # [seq_length, features]\n",
    "            seq_length, features = time_series.shape\n",
    "            batch_size = None\n",
    "        else:\n",
    "            # [batch_size, seq_length, features]\n",
    "            batch_size, seq_length, features = time_series.shape\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        num_patches = (seq_length - self.patch_length) // self.stride + 1\n",
    "        \n",
    "        if batch_size is None:\n",
    "            # Initialize patches array\n",
    "            patches = np.zeros((num_patches, self.patch_length, features))\n",
    "            \n",
    "            # Create patches\n",
    "            for i in range(num_patches):\n",
    "                start_idx = i * self.stride\n",
    "                end_idx = start_idx + self.patch_length\n",
    "                patches[i] = time_series[start_idx:end_idx]\n",
    "                \n",
    "            # Restore original dimensionality if input was 1D\n",
    "            if len(original_shape) == 1:\n",
    "                patches = patches.reshape(num_patches, self.patch_length)\n",
    "        else:\n",
    "            # Initialize patches array for batched data\n",
    "            patches = np.zeros((batch_size, num_patches, self.patch_length, features))\n",
    "            \n",
    "            # Create patches\n",
    "            for b in range(batch_size):\n",
    "                for i in range(num_patches):\n",
    "                    start_idx = i * self.stride\n",
    "                    end_idx = start_idx + self.patch_length\n",
    "                    patches[b, i] = time_series[b, start_idx:end_idx]\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def merge_patches(self, patches: np.ndarray, original_length: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Merge patches back into a time series.\n",
    "        For overlapping regions, values are averaged.\n",
    "        \n",
    "        Args:\n",
    "            patches: Patches of shape [batch_size, num_patches, patch_length, features]\n",
    "                     or [num_patches, patch_length, features]\n",
    "                     or [num_patches, patch_length]\n",
    "            original_length: Original sequence length (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed time series\n",
    "        \"\"\"\n",
    "        # Handle different input shapes\n",
    "        original_shape = patches.shape\n",
    "        if len(original_shape) == 2:\n",
    "            # [num_patches, patch_length] -> [num_patches, patch_length, 1]\n",
    "            patches = patches.reshape(original_shape[0], original_shape[1], 1)\n",
    "            num_patches, patch_length, features = patches.shape\n",
    "            batch_size = None\n",
    "        elif len(original_shape) == 3:\n",
    "            # [num_patches, patch_length, features]\n",
    "            num_patches, patch_length, features = patches.shape\n",
    "            batch_size = None\n",
    "        else:\n",
    "            # [batch_size, num_patches, patch_length, features]\n",
    "            batch_size, num_patches, patch_length, features = patches.shape\n",
    "        \n",
    "        # Calculate reconstructed sequence length\n",
    "        if original_length is None:\n",
    "            seq_length = (num_patches - 1) * self.stride + patch_length\n",
    "        else:\n",
    "            seq_length = original_length\n",
    "        \n",
    "        if batch_size is None:\n",
    "            # Initialize reconstructed time series and count array for averaging\n",
    "            reconstructed = np.zeros((seq_length, features))\n",
    "            counts = np.zeros((seq_length, features))\n",
    "            \n",
    "            # Merge patches\n",
    "            for i in range(num_patches):\n",
    "                start_idx = i * self.stride\n",
    "                end_idx = start_idx + patch_length\n",
    "                reconstructed[start_idx:end_idx] += patches[i]\n",
    "                counts[start_idx:end_idx] += 1\n",
    "                \n",
    "            # Average overlapping regions\n",
    "            reconstructed = reconstructed / np.maximum(counts, 1)\n",
    "            \n",
    "            # Restore original dimensionality if input was 2D\n",
    "            if len(original_shape) == 2:\n",
    "                reconstructed = reconstructed.reshape(seq_length)\n",
    "        else:\n",
    "            # Initialize reconstructed time series and count array for batched data\n",
    "            reconstructed = np.zeros((batch_size, seq_length, features))\n",
    "            counts = np.zeros((batch_size, seq_length, features))\n",
    "            \n",
    "            # Merge patches\n",
    "            for b in range(batch_size):\n",
    "                for i in range(num_patches):\n",
    "                    start_idx = i * self.stride\n",
    "                    end_idx = start_idx + patch_length\n",
    "                    reconstructed[b, start_idx:end_idx] += patches[b, i]\n",
    "                    counts[b, start_idx:end_idx] += 1\n",
    "                    \n",
    "            # Average overlapping regions\n",
    "            reconstructed = reconstructed / np.maximum(counts, 1)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "# --- 2. ABBASymbolicAggregation ---\n",
    "class ABBASymbolicAggregation:\n",
    "    \"\"\"\n",
    "    Implementation of ABBA (Aggregation-Based Amplitude Scaling) for symbolic pattern extraction (Step 3 in HSQP).\n",
    "    This is a simplified implementation based on the fABBA library concepts.\n",
    "    \"\"\"\n",
    "    def __init__(self, tol: float = 0.1, alpha: float = 0.1, sorting: str = '2-norm', scl: float = 1, k: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize ABBA parameters.\n",
    "        \n",
    "        Args:\n",
    "            tol: Tolerance for compression\n",
    "            alpha: Parameter for digitization\n",
    "            sorting: Method for sorting ('2-norm', 'area', etc.)\n",
    "            scl: Scaling factor\n",
    "            k: Number of symbols/clusters\n",
    "        \"\"\"\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.sorting = sorting\n",
    "        self.scl = scl\n",
    "        self.k = k\n",
    "        self.parameters = None\n",
    "        self.kmeans = None\n",
    "        \n",
    "    def compress(self, ts: np.ndarray) -> List[Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Compress time series into piecewise linear segments (polygonal chain).\n",
    "        \n",
    "        Args:\n",
    "            ts: Time series data\n",
    "            \n",
    "        Returns:\n",
    "            List of (len, inc) tuples representing the polygonal segments\n",
    "        \"\"\"\n",
    "        # Ensure ts is a 1D array\n",
    "        ts = np.asarray(ts).flatten()\n",
    "        n = len(ts)\n",
    "        \n",
    "        # Initialize\n",
    "        pieces = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        while start_idx < n - 1:\n",
    "            # Find the longest possible segment within tolerance\n",
    "            end_idx = start_idx + 1\n",
    "            while end_idx < n:\n",
    "                # Create a line from start to current end\n",
    "                if end_idx == start_idx + 1:\n",
    "                    line_segment = np.array([ts[start_idx], ts[end_idx]])\n",
    "                else:\n",
    "                    t = np.linspace(0, 1, end_idx - start_idx + 1)\n",
    "                    line_segment = ts[start_idx] + (ts[end_idx] - ts[start_idx]) * t\n",
    "                \n",
    "                # Check if the approximation is within tolerance\n",
    "                if np.max(np.abs(line_segment - ts[start_idx:end_idx+1])) <= self.tol:\n",
    "                    end_idx += 1\n",
    "                else:\n",
    "                    end_idx -= 1\n",
    "                    break\n",
    "            \n",
    "            # If we've reached the end of the time series\n",
    "            if end_idx >= n:\n",
    "                end_idx = n - 1\n",
    "            \n",
    "            # Calculate length and increment of the segment\n",
    "            length = end_idx - start_idx\n",
    "            increment = ts[end_idx] - ts[start_idx]\n",
    "            \n",
    "            # Add the segment to pieces\n",
    "            pieces.append((length, increment))\n",
    "            \n",
    "            # Move to the next segment\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        return pieces\n",
    "    \n",
    "    def digitize(self, pieces: List[Tuple[float, float]]) -> Tuple[List[str], Dict]:\n",
    "        \"\"\"\n",
    "        Convert polygonal segments into symbolic representation.\n",
    "        \n",
    "        Args:\n",
    "            pieces: List of (len, inc) tuples\n",
    "            \n",
    "        Returns:\n",
    "            string: List of symbols\n",
    "            parameters: Dictionary of parameters for inverse transformation\n",
    "        \"\"\"\n",
    "        # Extract features from pieces\n",
    "        features = np.array(pieces)\n",
    "        \n",
    "        # Normalize features if needed\n",
    "        if self.scl != 1:\n",
    "            features = features / self.scl\n",
    "        \n",
    "        # Cluster the features\n",
    "        if self.kmeans is None:\n",
    "            # Ensure there are enough samples for clustering\n",
    "            if len(features) < self.k:\n",
    "                # Fallback: if not enough data, just use a single symbol 'a'\n",
    "                symbols = ['a'] * len(features)\n",
    "                self.parameters = {\n",
    "                    'centers': np.array([[0.0, 0.0]]), # Placeholder\n",
    "                    'scl': self.scl,\n",
    "                    'alpha': self.alpha\n",
    "                }\n",
    "                return symbols, self.parameters\n",
    "\n",
    "            self.kmeans = KMeans(n_clusters=self.k, random_state=0, n_init='auto')\n",
    "            self.kmeans.fit(features)\n",
    "        \n",
    "        # Get cluster assignments\n",
    "        labels = self.kmeans.predict(features)\n",
    "        \n",
    "        # Convert to string representation (a, b, c, ...)\n",
    "        symbols = [chr(97 + label) for label in labels]\n",
    "        \n",
    "        # Store parameters for inverse transformation\n",
    "        self.parameters = {\n",
    "            'centers': self.kmeans.cluster_centers_,\n",
    "            'scl': self.scl,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "        \n",
    "        return symbols, self.parameters\n",
    "    \n",
    "    def fit_transform(self, ts: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Apply ABBA transformation to time series.\n",
    "        \n",
    "        Args:\n",
    "            ts: Time series data\n",
    "            \n",
    "        Returns:\n",
    "            Symbolic representation of the time series\n",
    "        \"\"\"\n",
    "        pieces = self.compress(ts)\n",
    "        symbols, _ = self.digitize(pieces)\n",
    "        return ''.join(symbols)\n",
    "    \n",
    "    def inverse_transform(self, string: str, initial_value: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert symbolic representation back to time series.\n",
    "        \n",
    "        Args:\n",
    "            string: Symbolic representation\n",
    "            initial_value: Initial value of the time series\n",
    "            \n",
    "        Returns:\n",
    "            Reconstructed time series\n",
    "        \"\"\"\n",
    "        if self.parameters is None:\n",
    "            raise ValueError(\"ABBA model must be fitted before inverse transform\")\n",
    "        \n",
    "        # Convert string to cluster indices\n",
    "        indices = [ord(s) - 97 for s in string]\n",
    "        \n",
    "        # Get cluster centers\n",
    "        centers = self.parameters['centers']\n",
    "        \n",
    "        # Scale back if needed\n",
    "        if self.scl != 1:\n",
    "            centers = centers * self.scl\n",
    "        \n",
    "        # Reconstruct pieces\n",
    "        pieces = [tuple(centers[idx]) for idx in indices]\n",
    "        \n",
    "        # Reconstruct time series\n",
    "        ts_recon = [initial_value]\n",
    "        for length, increment in pieces:\n",
    "            # Convert float length to integer\n",
    "            length = int(round(length))\n",
    "            if length < 1:\n",
    "                length = 1\n",
    "                \n",
    "            # Create linear segment\n",
    "            if length == 1:\n",
    "                ts_recon.append(ts_recon[-1] + increment)\n",
    "            else:\n",
    "                # Linear interpolation for the segment\n",
    "                start_val = ts_recon[-1]\n",
    "                end_val = start_val + increment\n",
    "                segment = np.linspace(start_val, end_val, length + 1)[1:] # Exclude start_val\n",
    "                ts_recon.extend(segment)\n",
    "        \n",
    "        return np.array(ts_recon)\n",
    "\n",
    "\n",
    "# --- 3. FeatureQuantization ---\n",
    "class FeatureQuantization:\n",
    "    \"\"\"\n",
    "    Quantization of ABBA-derived features for efficiency optimization (Step 4 in HSQP).\n",
    "    \"\"\"\n",
    "    def __init__(self, bit_width: int = 8, method: str = 'affine', block_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize quantization parameters.\n",
    "        \n",
    "        Args:\n",
    "            bit_width: Target bit width (e.g., 8 for INT8, 4 for INT4)\n",
    "            method: Quantization method ('affine', 'abs_max')\n",
    "            block_size: Block size for block-wise quantization (not fully implemented here, kept for API)\n",
    "        \"\"\"\n",
    "        self.bit_width = bit_width\n",
    "        self.method = method\n",
    "        self.block_size = block_size\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        \n",
    "        # Calculate quantization range\n",
    "        self.qmin = -(2 ** (bit_width - 1))\n",
    "        self.qmax = 2 ** (bit_width - 1) - 1\n",
    "        \n",
    "    def quantize(self, features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Quantize features to lower precision.\n",
    "        \n",
    "        Args:\n",
    "            features: Input features\n",
    "            \n",
    "        Returns:\n",
    "            Quantized features\n",
    "        \"\"\"\n",
    "        if self.method == 'abs_max':\n",
    "            # Absolute max quantization\n",
    "            abs_max = np.max(np.abs(features))\n",
    "            if abs_max == 0:\n",
    "                abs_max = 1.0  # Avoid division by zero\n",
    "                \n",
    "            self.scale = self.qmax / abs_max\n",
    "            self.zero_point = 0\n",
    "            \n",
    "            # Quantize\n",
    "            q_features = np.round(features * self.scale)\n",
    "            q_features = np.clip(q_features, self.qmin, self.qmax)\n",
    "            \n",
    "        elif self.method == 'affine':\n",
    "            # Affine quantization\n",
    "            f_min = np.min(features)\n",
    "            f_max = np.max(features)\n",
    "            \n",
    "            if f_min == f_max:\n",
    "                self.scale = 1.0\n",
    "                self.zero_point = 0\n",
    "            else:\n",
    "                self.scale = (self.qmax - self.qmin) / (f_max - f_min)\n",
    "                self.zero_point = self.qmin - round(f_min * self.scale)\n",
    "            \n",
    "            # Quantize\n",
    "            q_features = np.round(features * self.scale + self.zero_point)\n",
    "            q_features = np.clip(q_features, self.qmin, self.qmax)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown quantization method: {self.method}\")\n",
    "        \n",
    "        return q_features.astype(np.int8 if self.bit_width <= 8 else np.int16)\n",
    "    \n",
    "    def dequantize(self, q_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Dequantize features back to original precision.\n",
    "        \n",
    "        Args:\n",
    "            q_features: Quantized features\n",
    "            \n",
    "        Returns:\n",
    "            Dequantized features\n",
    "        \"\"\"\n",
    "        if self.scale is None or (self.method == 'affine' and self.zero_point is None):\n",
    "            raise ValueError(\"Quantization parameters not set. Call quantize() first.\")\n",
    "        \n",
    "        if self.method == 'abs_max':\n",
    "            return q_features / self.scale\n",
    "        elif self.method == 'affine':\n",
    "            return (q_features - self.zero_point) / self.scale\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown quantization method: {self.method}\")\n",
    "\n",
    "\n",
    "# --- 4. HSQP (Main Orchestrator) ---\n",
    "class HSQP:\n",
    "    \"\"\"\n",
    "    Hierarchical Symbolic-Quantized Patching (HSQP) for time-series tokenization.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 patch_length: int = 24, \n",
    "                 stride: int = 12,\n",
    "                 tol: float = 0.1, \n",
    "                 alpha: float = 0.1, \n",
    "                 k: int = 26,  # Limited to 26 for a-z symbols\n",
    "                 bit_width: int = 8,\n",
    "                 quant_method: str = 'affine',\n",
    "                 embedding_dim: int = 64):\n",
    "        \"\"\"\n",
    "        Initialize HSQP parameters.\n",
    "        \n",
    "        Args:\n",
    "            patch_length: Length of each patch\n",
    "            stride: Step size between patches\n",
    "            tol: Tolerance for ABBA compression\n",
    "            alpha: Parameter for ABBA digitization\n",
    "            k: Number of symbols/clusters for ABBA\n",
    "            bit_width: Target bit width for quantization\n",
    "            quant_method: Quantization method\n",
    "            embedding_dim: Dimension for LLM embedding\n",
    "        \"\"\"\n",
    "        self.patching = TimeSeriesPatching(patch_length=patch_length, stride=stride)\n",
    "        self.abba = ABBASymbolicAggregation(tol=tol, alpha=alpha, k=k)\n",
    "        self.quantization = FeatureQuantization(bit_width=bit_width, method=quant_method)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # For LLM embedding\n",
    "        self.embedding = None\n",
    "        \n",
    "    def fit_transform(self, time_series: np.ndarray) -> Tuple[List[str], np.ndarray, List[List[Tuple[float, float]]]]:\n",
    "        \"\"\"\n",
    "        Apply HSQP transformation to time series.\n",
    "        \n",
    "        Args:\n",
    "            time_series: Input time series data\n",
    "            \n",
    "        Returns:\n",
    "            symbols_list: List of symbolic representations for each patch\n",
    "            quantized_features: Quantized ABBA-derived features\n",
    "            pieces_list: List of polygonal segments for each patch\n",
    "        \"\"\"\n",
    "        # Step 2: Initial Patching\n",
    "        patches = self.patching.create_patches(time_series)\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if len(patches.shape) == 4:  # [batch_size, num_patches, patch_length, features]\n",
    "            batch_size, num_patches = patches.shape[0], patches.shape[1]\n",
    "            is_batched = True\n",
    "        else:  # [num_patches, patch_length, features] or [num_patches, patch_length]\n",
    "            num_patches = patches.shape[0]\n",
    "            is_batched = False\n",
    "        \n",
    "        # Step 3: ABBA Symbolic Aggregation\n",
    "        symbols_list = []\n",
    "        pieces_list = []\n",
    "        \n",
    "        if is_batched:\n",
    "            for b in range(batch_size):\n",
    "                batch_symbols = []\n",
    "                batch_pieces = []\n",
    "                for i in range(num_patches):\n",
    "                    # Extract patch\n",
    "                    if len(patches.shape) == 4:  # [batch_size, num_patches, patch_length, features]\n",
    "                        patch = patches[b, i, :, 0]  # Using first feature for simplicity\n",
    "                    \n",
    "                    # Apply ABBA\n",
    "                    pieces = self.abba.compress(patch)\n",
    "                    symbols, _ = self.abba.digitize(pieces)\n",
    "                    \n",
    "                    batch_symbols.append(''.join(symbols))\n",
    "                    batch_pieces.append(pieces)\n",
    "                \n",
    "                symbols_list.append(batch_symbols)\n",
    "                pieces_list.append(batch_pieces)\n",
    "        else:\n",
    "            for i in range(num_patches):\n",
    "                # Extract patch\n",
    "                if len(patches.shape) == 3:  # [num_patches, patch_length, features]\n",
    "                    patch = patches[i, :, 0]  # Using first feature for simplicity\n",
    "                else:  # [num_patches, patch_length]\n",
    "                    patch = patches[i]\n",
    "                \n",
    "                # Apply ABBA\n",
    "                pieces = self.abba.compress(patch)\n",
    "                symbols, _ = self.abba.digitize(pieces)\n",
    "                \n",
    "                symbols_list.append(''.join(symbols))\n",
    "                pieces_list.append(pieces)\n",
    "        \n",
    "        # Step 4: Quantization of ABBA-Derived Features\n",
    "        # Extract features from pieces\n",
    "        if is_batched:\n",
    "            all_features = []\n",
    "            for batch_pieces in pieces_list:\n",
    "                batch_features = []\n",
    "                for pieces in batch_pieces:\n",
    "                    # Handle case where pieces is empty\n",
    "                    if not pieces:\n",
    "                        batch_features.append(np.zeros((1, 2))) # Placeholder for empty patch\n",
    "                    else:\n",
    "                        batch_features.append(np.array(pieces))\n",
    "                all_features.append(np.vstack(batch_features))\n",
    "            features = np.vstack(all_features)\n",
    "        else:\n",
    "            all_features = []\n",
    "            for pieces in pieces_list:\n",
    "                # Handle case where pieces is empty\n",
    "                if not pieces:\n",
    "                    all_features.append(np.zeros((1, 2))) # Placeholder for empty patch\n",
    "                else:\n",
    "                    all_features.append(np.array(pieces))\n",
    "            features = np.vstack(all_features)\n",
    "        \n",
    "        # Quantize features\n",
    "        quantized_features = self.quantization.quantize(features)\n",
    "        \n",
    "        return symbols_list, quantized_features, pieces_list\n",
    "    \n",
    "    def create_llm_embeddings(self, quantized_features: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create LLM embeddings from quantized features (Step 5 in HSQP).\n",
    "        \n",
    "        Args:\n",
    "            quantized_features: Quantized ABBA-derived features\n",
    "            \n",
    "        Returns:\n",
    "            Embeddings for LLM\n",
    "        \"\"\"\n",
    "        # Initialize embedding layer if not already created\n",
    "        if self.embedding is None:\n",
    "            # The quantized features are 2D (length, 2 features: length, increment)\n",
    "            # We need to map this to the embedding_dim\n",
    "            # A simple linear layer can serve as the embedding\n",
    "            # The input size is 2 (length, increment)\n",
    "            self.embedding = nn.Linear(2, self.embedding_dim)\n",
    "            \n",
    "        # Convert to torch tensor and float\n",
    "        q_features_tensor = torch.from_numpy(quantized_features).float()\n",
    "        \n",
    "        # Pass through the linear embedding layer\n",
    "        embeddings = self.embedding(q_features_tensor)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def inverse_transform(self, embeddings: torch.Tensor, original_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Inverse transform from LLM embeddings back to time series.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: LLM embeddings (output of the LLM)\n",
    "            original_length: Original sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Dequantized features (length, increment) - simplified output for plugin context.\n",
    "        \"\"\"\n",
    "        if self.embedding is None:\n",
    "            raise ValueError(\"Embedding layer not initialized. Call create_llm_embeddings() first.\")\n",
    "            \n",
    "        # Placeholder for dequantized features\n",
    "        return np.zeros((1, 2))\n",
    "\n",
    "\n",
    "# --- 5. HSQP Plugin for PatchTST ---\n",
    "class HSQP_PatchTST_Plugin(nn.Module):\n",
    "    \"\"\"\n",
    "    HSQP Plugin to replace the standard PatchTST Patching/Embedding layer.\n",
    "    \n",
    "    Input: [B, L, C]\n",
    "    Output: [B * C, Num_Tokens, D_Model]\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(HSQP_PatchTST_Plugin, self).__init__()\n",
    "        \n",
    "        # HSQP Parameters\n",
    "        self.patch_length = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.embedding_dim = configs.d_model\n",
    "        self.num_channels = configs.c_in # Number of input features/channels\n",
    "        \n",
    "        # HSQP Initialization (one instance per channel for independent processing)\n",
    "        # Using nn.ModuleList to ensure parameters are registered\n",
    "        self.hsqp_channels = nn.ModuleList([ \n",
    "            HSQP(\n",
    "                patch_length=self.patch_length,\n",
    "                stride=self.stride,\n",
    "                embedding_dim=self.embedding_dim,\n",
    "                k=getattr(configs, 'hsqp_k', 26),\n",
    "                tol=getattr(configs, 'hsqp_tol', 0.1),\n",
    "                bit_width=getattr(configs, 'hsqp_bit_width', 8),\n",
    "                quant_method=getattr(configs, 'hsqp_quant_method', 'affine')\n",
    "            ) for _ in range(self.num_channels)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the HSQP plugin.\n",
    "        \n",
    "        Args:\n",
    "            x: Input time series tensor of shape [Batch, Seq_Len, Channels]\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings for the Transformer encoder of shape [Batch * Channels, Num_Tokens, D_Model]\n",
    "            n_vars: Number of channels (C)\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape # Batch, Seq_Len, Channels\n",
    "        \n",
    "        all_channel_embeddings = []\n",
    "        \n",
    "        # Process each channel independently (Channel-Independence in PatchTST)\n",
    "        for c in range(C):\n",
    "            hsqp_processor = self.hsqp_channels[c]\n",
    "            \n",
    "            # Extract channel data: [B, L]\n",
    "            x_channel = x[:, :, c].cpu().numpy()\n",
    "            \n",
    "            all_batch_embeddings = []\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for i in range(B):\n",
    "                # 1. HSQP Transformation (Patching, ABBA, Quantization)\n",
    "                # The input to fit_transform is [L]\n",
    "                _, quantized_features, _ = hsqp_processor.fit_transform(x_channel[i])\n",
    "                \n",
    "                # 2. Create LLM Embeddings\n",
    "                # Output shape: [Num_Segments, D_Model]\n",
    "                embeddings = hsqp_processor.create_llm_embeddings(quantized_features)\n",
    "                \n",
    "                all_batch_embeddings.append(embeddings)\n",
    "                \n",
    "            # Find max length in the current batch for this channel\n",
    "            max_len = max(e.shape[0] for e in all_batch_embeddings)\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_embeddings = []\n",
    "            for e in all_batch_embeddings:\n",
    "                padding_needed = max_len - e.shape[0]\n",
    "                if padding_needed > 0:\n",
    "                    # Pad with zeros\n",
    "                    padding = torch.zeros(padding_needed, self.embedding_dim, device=e.device)\n",
    "                    e = torch.cat([e, padding], dim=0)\n",
    "                padded_embeddings.append(e)\n",
    "                \n",
    "            # Stack the batch embeddings: [B, Num_Segments, D_Model]\n",
    "            channel_embeddings = torch.stack(padded_embeddings, dim=0).to(x.device)\n",
    "            \n",
    "            all_channel_embeddings.append(channel_embeddings)\n",
    "            \n",
    "        # Concatenate all channels: [B * C, Num_Segments, D_Model]\n",
    "        # First, stack: [C, B, Num_Segments, D_Model]\n",
    "        stacked_embeddings = torch.stack(all_channel_embeddings, dim=0)\n",
    "        # Then, reshape: [B * C, Num_Segments, D_Model]\n",
    "        output_embeddings = stacked_embeddings.permute(1, 0, 2, 3).reshape(B * C, -1, self.embedding_dim)\n",
    "        \n",
    "        return output_embeddings, C\n",
    "\n",
    "\n",
    "# --- End of HSQP Plugin Implementation ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PatchTST Model Modification\n",
    "\n",
    "The `PatchTST_backbone.py` file is modified to conditionally use the `HSQP_PatchTST_Plugin` instead of the standard patching logic when the `use_hsqp` flag is set in the configuration. This addresses the reviewer's concern about making it a plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile layers/PatchTST_backbone.py\n",
    "__all__ = ['PatchTST_backbone']\n",
    "\n",
    "# Cell\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#from collections import OrderedDict\n",
    "from layers.PatchTST_layers import *\n",
    "from layers.RevIN import RevIN\n",
    "from models.hsqp_plugin import HSQP_PatchTST_Plugin # Import HSQP Plugin\n",
    "\n",
    "# Cell\n",
    "class PatchTST_backbone(nn.Module):\n",
    "    def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int, stride:int, max_seq_len:Optional[int]=1024, \n",
    "                 n_layers:int=3, d_model=128, n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None,\n",
    "                 d_ff:int=256, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", key_padding_mask:bool='auto',\n",
    "                 padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n",
    "                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., head_dropout = 0, padding_patch = None,\n",
    "                 use_hsqp:bool=False, # New argument for HSQP\n",
    "                 hsqp_k:int=26, hsqp_tol:float=0.1, hsqp_bit_width:int=8, hsqp_quant_method:str='affine', # HSQP args\n",
    "                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False,\n",
    "                 verbose:bool=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Store configs for dynamic initialization\n",
    "        self.c_in = c_in\n",
    "        self.context_window = context_window\n",
    "        self.target_window = target_window\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_ff = d_ff\n",
    "        self.norm = norm\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.key_padding_mask = key_padding_mask\n",
    "        self.padding_var = padding_var\n",
    "        self.attn_mask = attn_mask\n",
    "        self.res_attention = res_attention\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "        self.pe = pe\n",
    "        self.learn_pe = learn_pe\n",
    "        self.verbose = verbose\n",
    "        self.head_dropout = head_dropout\n",
    "        self.individual = individual\n",
    "        \n",
    "        # RevIn\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "        \n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        self.use_hsqp = use_hsqp\n",
    "        \n",
    "        if self.use_hsqp:\n",
    "            self.hsqp_layer = HSQP_PatchTST_Plugin(self) # Pass self (configs) to the plugin\n",
    "            patch_num = None # Patch num is dynamic with HSQP\n",
    "        else:\n",
    "            patch_num = int((context_window - patch_len)/stride + 1)\n",
    "            if padding_patch == 'end': # can be modified to general case\n",
    "                self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) \n",
    "                patch_num += 1\n",
    "        \n",
    "        # Backbone \n",
    "        if self.use_hsqp:\n",
    "            # TSTiEncoder will be initialized in forward pass after patch_num is determined\n",
    "            self.backbone = None\n",
    "        else:\n",
    "            self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
    "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
    "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n",
    "\n",
    "        # Head\n",
    "        self.head_nf = d_model * patch_num if patch_num is not None else None\n",
    "        self.n_vars = c_in\n",
    "        self.pretrain_head = pretrain_head\n",
    "        self.head_type = head_type\n",
    "        \n",
    "        if self.pretrain_head: \n",
    "            self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) # custom head passed as a partial func with all its kwargs\n",
    "        elif head_type == 'flatten': \n",
    "            if not self.use_hsqp:\n",
    "                self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
    "            else:\n",
    "                self.head = None # Will be initialized in forward pass\n",
    "        \n",
    "        self.patch_num = patch_num # Store patch_num for TSTiEncoder init\n",
    "        \n",
    "    \n",
    "    def forward(self, z):                                                                   # z: [bs x nvars x seq_len]\n",
    "        # norm\n",
    "        if self.revin: \n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'norm')\n",
    "            z = z.permute(0,2,1)\n",
    "            \n",
    "        # do patching\n",
    "        if self.use_hsqp:\n",
    "            # HSQP Plugin: z: [bs x nvars x seq_len] -> z: [bs * nvars x patch_num x d_model], n_vars\n",
    "            # The plugin expects [B, L, C] or [B, C, L] depending on implementation. PatchTST uses [B, C, L].\n",
    "            # The HSQP plugin was designed for [B, L, C] in the previous step, so we permute back to [B, L, C] for the plugin\n",
    "            # and then permute back to [B, C, L] for the rest of the model if needed.\n",
    "            # Let's adjust the plugin to take [B, C, L] to match PatchTST's internal flow.\n",
    "            # Re-checking the plugin: it takes [B, L, C] and returns [B*C, Num_Tokens, D_Model], C\n",
    "            # PatchTST's input is [B, C, L]. Let's permute to [B, L, C] for the plugin.\n",
    "            z_permuted = z.permute(0, 2, 1).contiguous() # [B, L, C]\n",
    "            z_enc, n_vars = self.hsqp_layer(z_permuted)\n",
    "            \n",
    "            # Dynamically initialize TSTiEncoder and Head if not done\n",
    "            if self.backbone is None:\n",
    "                patch_num = z_enc.shape[1] # Patch num is the sequence length of the tokens\n",
    "                self.backbone = TSTiEncoder(self.c_in, patch_num=patch_num, patch_len=self.patch_len, max_seq_len=1024,\n",
    "                                        n_layers=self.n_layers, d_model=self.d_model, n_heads=self.n_heads, d_k=self.d_k, d_v=self.d_v, d_ff=self.d_ff,\n",
    "                                        attn_dropout=self.attn_dropout, dropout=self.dropout, act=self.act, key_padding_mask=self.key_padding_mask, padding_var=self.padding_var,\n",
    "                                        attn_mask=self.attn_mask, res_attention=self.res_attention, pre_norm=self.pre_norm, store_attn=self.store_attn,\n",
    "                                        pe=self.pe, learn_pe=self.learn_pe, verbose=self.verbose)\n",
    "                \n",
    "                self.head_nf = self.d_model * patch_num\n",
    "                self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, self.target_window, head_dropout=self.head_dropout)\n",
    "            \n",
    "            # model\n",
    "            # TSTiEncoder expects [bs * nvars x patch_num x d_model]\n",
    "            z = self.backbone(z_enc) # z: [bs * nvars x patch_num x d_model] -> z: [bs x nvars x d_model x patch_num]\n",
    "            z = self.head(z) # z: [bs x nvars x target_window]\n",
    "            \n",
    "        else:\n",
    "            # Standard PatchTST logic\n",
    "            if self.padding_patch == 'end':\n",
    "                z = self.padding_patch_layer(z)\n",
    "            z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
    "            z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
    "            \n",
    "            # model\n",
    "            z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
    "            z = self.head(z)                                                                    # z: [bs x nvars x target_window]\n",
    "        \n",
    "        # denorm\n",
    "        if self.revin: \n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'denorm')\n",
    "            z = z.permute(0,2,1)\n",
    "        return z\n",
    "    \n",
    "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
    "        return nn.Sequential(nn.Dropout(dropout),\n",
    "                    nn.Conv1d(head_nf, vars, 1)\n",
    "                    )\n",
    "\n",
    "\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "        \n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class TSTiEncoder(nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        \n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "        \n",
    "    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num] or [bs * nvars x patch_num x d_model] (for HSQP)\n",
    "        \n",
    "        # Check if input is already embedded (HSQP case)\n",
    "        if x.dim() == 3: # [bs * nvars x patch_num x d_model]\n",
    "            u = x\n",
    "            # Reshape back to [bs x nvars x patch_num x d_model] for positional encoding\n",
    "            # This is a hacky fix for the TSTiEncoder structure, but necessary for dynamic patch_num\n",
    "            # We need to know B and n_vars from the caller, but TSTiEncoder doesn't have it.\n",
    "            # The original PatchTST uses [bs * nvars x patch_num x d_model] after W_P.\n",
    "            # Let's assume the input is already in the correct shape for the encoder part.\n",
    "            # The input to TSTiEncoder is [bs * nvars x patch_num x d_model] after W_P and reshape.\n",
    "            # The HSQP plugin already returns [B * C, Num_Tokens, D_Model], which is the correct shape for the encoder.\n",
    "            \n",
    "            # Positional encoding and dropout\n",
    "            u = self.dropout(u + self.W_pos)\n",
    "            \n",
    "            # Encoder\n",
    "            z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "            \n",
    "            # Reshape for head\n",
    "            # The head expects [bs x nvars x d_model x patch_num]\n",
    "            # We need B and n_vars from the caller (PatchTST_backbone)\n",
    "            # Since TSTiEncoder is now dynamically initialized, we need to pass B and n_vars from PatchTST_backbone\n",
    "            # This is getting too complex for a quick plugin. Let's simplify the TSTiEncoder for HSQP.\n",
    "            # The PatchTST_backbone already handles the reshape back to [bs x nvars x d_model x patch_num] for the head.\n",
    "            # TSTiEncoder's job is to take [bs * nvars x patch_num x d_model] and return [bs * nvars x patch_num x d_model]\n",
    "            \n",
    "            # The original TSTiEncoder is designed to take [bs x nvars x patch_len x patch_num] and return [bs x nvars x d_model x patch_num]\n",
    "            # Let's keep the original TSTiEncoder logic and adjust the PatchTST_backbone to handle the HSQP output.\n",
    "            \n",
    "            # Revert TSTiEncoder to original logic and remove the HSQP-specific logic here.\n",
    "            # The HSQP logic is handled in PatchTST_backbone.\n",
    "            \n",
    "            # TSTiEncoder (Original Logic)\n",
    "            n_vars = x.shape[1]\n",
    "            # Input encoding\n",
    "            x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "            x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "\n",
    "            u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "            u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "            # Encoder\n",
    "            z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "            z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "            z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "            \n",
    "            return z\n",
    "        \n",
    "        # TSTiEncoder (HSQP Logic - simplified)\n",
    "        else: # x is [bs * nvars x patch_num x d_model] from HSQP\n",
    "            u = x\n",
    "            # Positional encoding and dropout\n",
    "            u = self.dropout(u + self.W_pos)\n",
    "            \n",
    "            # Encoder\n",
    "            z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "            \n",
    "            # We return z as [bs * nvars x patch_num x d_model] and let PatchTST_backbone handle the final reshape\n",
    "            return z\n",
    "\n",
    "\n",
    "class TSTEncoder(nn.Module):\n",
    "    # ... (rest of the TSTEncoder and TSTEncoderLayer classes are unchanged)\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      activation=activation, res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "\n",
    "\n",
    "class TSTEncoderLayer(nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                get_activation_fn(activation),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            scores = None\n",
    "\n",
    "        if self.store_attn: \n",
    "            self.attn = attn\n",
    "        \n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2)\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        \n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2)\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        return src, scores\n",
    "\n",
    "\n",
    "# --- End of PatchTST_backbone.py Modification ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Experiment Loop\n",
    "\n",
    "This section includes instructions for uploading the datasets and the main loop to run the experiments one dataset at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Upload Datasets\n",
    "\n",
    "**Action Required:** Please upload your CSV files (`electricity.csv`, `ETTh1.csv`, `ETTh2.csv`, `ETTm1.csv`, `ETTm2.csv`, `weather.csv`, `traffic.csv`, and `national_illness.csv`) to the Colab environment's file system. A common practice is to create a `data` folder inside the `PatchTST_supervised` directory and place them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory and move files (assuming they are uploaded to the root)\n",
    "!mkdir -p data\n",
    "# If you uploaded the files to the root of the Colab environment, run the following:\n",
    "# !mv ../../<your_uploaded_file>.csv data/\n",
    "\n",
    "print(\"Please ensure your datasets are in the 'data' folder inside the 'PatchTST_supervised' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Experiment Configuration and Execution\n",
    "\n",
    "We will define a configuration class and a main function to run the experiments. We will iterate over the datasets, running the experiment for each one, and collecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from exp.exp_main import Exp_Main\n",
    "from utils.tools import setting\n",
    "\n",
    "# Define the list of datasets\n",
    "DATASETS = [\n",
    "    'electricity', 'ETTh1', 'ETTh2', 'ETTm1', 'ETTm2', 'weather', 'traffic', 'national_illness'\n",
    "]\n",
    "\n",
    "def get_default_args(dataset_name, use_hsqp=False):\n",
    "    \"\"\"Generates a default set of arguments for the experiment.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='PatchTST Forecasting')\n",
    "    \n",
    "    # Basic Config\n",
    "    parser.add_argument('--model', type=str, default='PatchTST', help='model name, PatchTST')\n",
    "    parser.add_argument('--data', type=str, default=dataset_name, help='dataset name')\n",
    "    parser.add_argument('--root_path', type=str, default='./data/', help='root path of the data file')\n",
    "    parser.add_argument('--data_path', type=str, default=f'{dataset_name}.csv', help='data file')\n",
    "    parser.add_argument('--features', type=str, default='M', help='forecasting task, options:[M, S, MS]; M:multivariate, S:univariate, MS:multivariate for S-model')\n",
    "    parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "    parser.add_argument('--freq', type=str, default='h', help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], A:annually, custom:freq(e.g. 15min)')\n",
    "    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "    # Forecasting Task\n",
    "    parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "    parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "    parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "    # Model Config\n",
    "    parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "    parser.add_argument('--d_model', type=int, default=128, help='dimension of model')\n",
    "    parser.add_argument('--d_ff', type=int, default=256, help='dimension of fcn')\n",
    "    parser.add_argument('--n_heads', type=int, default=16, help='num of heads')\n",
    "    parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "    parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "    parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "    parser.add_argument('--individual', action='store_true', default=False, help='DLinear: Individual or not')\n",
    "    parser.add_argument('--revin', action='store_true', default=True, help='RevIN')\n",
    "    parser.add_argument('--affine', action='store_true', default=True, help='RevIN-affine')\n",
    "    parser.add_argument('--subtract_last', action='store_true', default=False, help='RevIN-subtract_last')\n",
    "    \n",
    "    # HSQP Config\n",
    "    parser.add_argument('--use_hsqp', action='store_true', default=use_hsqp, help='whether to use HSQP plugin')\n",
    "    parser.add_argument('--hsqp_k', type=int, default=26, help='number of symbols for ABBA')\n",
    "    parser.add_argument('--hsqp_tol', type=float, default=0.1, help='tolerance for ABBA compression')\n",
    "    parser.add_argument('--hsqp_bit_width', type=int, default=8, help='bit width for quantization')\n",
    "    parser.add_argument('--hsqp_quant_method', type=str, default='affine', help='quantization method')\n",
    "    \n",
    "    # Training Config\n",
    "    parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "    parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "    parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "    parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "    parser.add_argument('--use_multi_gpu', action='store_true', default=False, help='use multiple gpus')\n",
    "    parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "    parser.add_argument('--seed', type=int, default=2021, help='random seed')\n",
    "    \n",
    "    # Other Config\n",
    "    parser.add_argument('--num_workers', type=int, default=10, help='num of workers')\n",
    "    parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "    parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "    parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "    parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "    parser.add_argument('--task_name', type=str, default='long_term_forecast', help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification]')\n",
    "    parser.add_argument('--train_only', action='store_true', help='train only', default=False)\n",
    "    \n",
    "    # Parse arguments and set defaults for Colab\n",
    "    args = parser.parse_args([]) # Pass empty list to avoid reading from command line\n",
    "    \n",
    "    # Adjust parameters based on dataset (as in original PatchTST)\n",
    "    if dataset_name == 'ETTh1' or dataset_name == 'ETTh2':\n",
    "        args.enc_in = 7\n",
    "    elif dataset_name == 'ETTm1' or dataset_name == 'ETTm2':\n",
    "        args.enc_in = 7\n",
    "    elif dataset_name == 'electricity':\n",
    "        args.enc_in = 321\n",
    "    elif dataset_name == 'traffic':\n",
    "        args.enc_in = 862\n",
    "    elif dataset_name == 'weather':\n",
    "        args.enc_in = 21\n",
    "    elif dataset_name == 'national_illness':\n",
    "        args.enc_in = 1\n",
    "        args.features = 'S'\n",
    "        args.target = 'y'\n",
    "        args.freq = 'w'\n",
    "        \n",
    "    # Set device\n",
    "    args.use_gpu = torch.cuda.is_available()\n",
    "    args.devices = '0'\n",
    "    \n",
    "    return args\n",
    "\n",
    "def run_experiment(dataset_name, use_hsqp):\n",
    "    \"\"\"Runs the PatchTST experiment for a single dataset and configuration.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting experiment for Dataset: {dataset_name}, HSQP: {use_hsqp}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    args = get_default_args(dataset_name, use_hsqp)\n",
    "    \n",
    "    # Set up the experiment environment\n",
    "    setting(args)\n",
    "    \n",
    "    # Initialize and run the experiment\n",
    "    Exp = Exp_Main\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # Train and Test\n",
    "    print('>>>>>>>start training>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    exp.train(setting)\n",
    "    \n",
    "    print('>>>>>>>start testing>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    mae, mse = exp.test(setting, test=1)\n",
    "    \n",
    "    print(f\"Experiment finished for {dataset_name}. MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
    "    \n",
    "    # Clean up to free memory before the next run\n",
    "    del exp\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return mae, mse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_results = []\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        # Run without HSQP (Baseline)\n",
    "        mae_base, mse_base = run_experiment(dataset, use_hsqp=False)\n",
    "        all_results.append({\n",
    "            'Dataset': dataset,\n",
    "            'Method': 'PatchTST (Baseline)',\n",
    "            'MAE': mae_base,\n",
    "            'MSE': mse_base\n",
    "        })\n",
    "        \n",
    "        # Run with HSQP\n",
    "        mae_hsqp, mse_hsqp = run_experiment(dataset, use_hsqp=True)\n",
    "        all_results.append({\n",
    "            'Dataset': dataset,\n",
    "            'Method': 'PatchTST + HSQP',\n",
    "            'MAE': mae_hsqp,\n",
    "            'MSE': mse_hsqp\n",
    "        })\n",
    "        \n",
    "    # Display final results\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\n\" + \"#\"*50)\n",
    "    print(\"Final Experiment Results\")\n",
    "    print(\"#\"*50)\n",
    "    print(results_df.to_markdown(index=False))\n",
    "    \n",
    "    # Save results to a file\n",
    "    results_df.to_csv('experiment_results.csv', index=False)\n",
    "    print(\"Results saved to experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Experiments\n",
    "\n",
    "After completing the setup and ensuring your datasets are in the `data` folder, run the following cell to start the full experiment loop. This will run the baseline and HSQP version for each dataset sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main experiment loop\n",
    "!python PatchTST_HSQP_Colab.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
