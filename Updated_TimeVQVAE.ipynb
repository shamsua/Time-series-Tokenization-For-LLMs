{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "import yaml\n",
        "import tempfile\n",
        "from einops import rearrange\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "def get_root_dir():\n",
        "    \"\"\"Returns the root directory for the project.\"\"\"\n",
        "    return Path(\"/content\")\n",
        "\n",
        "\n",
        "def load_yaml_param_settings(yaml_fname: str):\n",
        "    \"\"\"Loads parameters from a YAML file.\"\"\"\n",
        "    with open(yaml_fname, \"r\") as stream:\n",
        "        config = yaml.load(stream, Loader=yaml.FullLoader)\n",
        "    return config\n",
        "\n",
        "\n",
        "def preprocess(df, scaler: MinMaxScaler, kind: str):\n",
        "    \"\"\"Normalizes and standardizes data using MinMaxScaler.\"\"\"\n",
        "    df = np.asarray(df, dtype=np.float32)\n",
        "\n",
        "    if len(df.shape) == 1:\n",
        "        raise ValueError(\"Data must be a 2-D array\")\n",
        "\n",
        "    if np.any(np.isnan(df).sum() != 0):\n",
        "        print(\"Data contains null values. Will be replaced with 0\")\n",
        "        df = np.nan_to_num(df)  # Pass df to nan_to_num\n",
        "\n",
        "    if kind == \"train\":\n",
        "        df = scaler.fit_transform(df)\n",
        "    elif kind == \"test\":\n",
        "        df = scaler.transform(df)\n",
        "    print(\"Data normalized\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def minibatch_slices_iterator(length, step_size, ignore_incomplete_batch=False):\n",
        "    \"\"\"Iterates through mini-batch slices.\"\"\"\n",
        "    start = 0\n",
        "    stop1 = (length // step_size) * step_size\n",
        "    while start < stop1:\n",
        "        yield slice(start, start + step_size, 1)\n",
        "        start += step_size\n",
        "    if not ignore_incomplete_batch and start < length:\n",
        "        yield slice(start, length, 1)\n",
        "\n",
        "\n",
        "class BatchSlidingWindow(object):\n",
        "    \"\"\"Class for obtaining mini-batch iterators of sliding windows.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, array_size, window_size, step_size, batch_size, excludes=None,\n",
        "        shuffle=False, ignore_incomplete_batch=False\n",
        "    ):\n",
        "        if window_size < 1:\n",
        "            raise ValueError(\"`window_size` must be at least 1\")\n",
        "        if array_size < window_size:\n",
        "            raise ValueError(\"`array_size` must be at least as large as `window_size`\")\n",
        "        if excludes is not None:\n",
        "            excludes = np.asarray(excludes, dtype=np.bool_)\n",
        "            expected_shape = (array_size,)\n",
        "            if excludes.shape != expected_shape:\n",
        "                raise ValueError(\n",
        "                    f\"The shape of `excludes` is expected to be {expected_shape}, but got {excludes.shape}\"\n",
        "                )\n",
        "\n",
        "        if excludes is not None:\n",
        "            mask = np.logical_not(excludes)\n",
        "        else:\n",
        "            mask = np.ones([array_size], dtype=np.bool_)\n",
        "        mask[: window_size - 1] = False\n",
        "        where_excludes = np.where(excludes)[0]\n",
        "        for k in range(1, window_size):\n",
        "            also_excludes = where_excludes + k\n",
        "            also_excludes = also_excludes[also_excludes < array_size]\n",
        "            mask[also_excludes] = False\n",
        "\n",
        "        indices = np.arange(array_size)[mask]\n",
        "        self._indices = indices.reshape([-1, 1])\n",
        "\n",
        "        self._offsets = np.arange(-window_size + 1, 1)\n",
        "\n",
        "        self._array_size = array_size\n",
        "        self._window_size = window_size\n",
        "        self._step_size = step_size\n",
        "        self._batch_size = batch_size\n",
        "        self._shuffle = shuffle\n",
        "        self._ignore_incomplete_batch = ignore_incomplete_batch\n",
        "\n",
        "    def get_iterator(self, arrays):\n",
        "        \"\"\"Iterate through the sliding windows of each array in `arrays`.\"\"\"\n",
        "        arrays = tuple(np.asarray(a) for a in arrays)\n",
        "        if not arrays:\n",
        "            raise ValueError(\"`arrays` must not be empty\")\n",
        "\n",
        "        if self._shuffle:\n",
        "            np.random.shuffle(self._indices)\n",
        "\n",
        "        for s in minibatch_slices_iterator(\n",
        "            length=len(self._indices),\n",
        "            step_size=self._step_size,\n",
        "            ignore_incomplete_batch=self._ignore_incomplete_batch,\n",
        "        ):\n",
        "            idx = self._indices[s] + self._offsets\n",
        "            yield tuple(a[idx] if len(a.shape) == 1 else a[idx, :] for a in arrays)\n",
        "\n",
        "\n",
        "def freeze(model):\n",
        "    \"\"\"Freezes the parameters of a PyTorch model.\"\"\"\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "def unfreeze(model):\n",
        "    \"\"\"Unfreezes the parameters of a PyTorch model.\"\"\"\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "def save_model(models_dict: dict, dirname=\"saved_models\", id: str = \"\"):\n",
        "    \"\"\"Saves PyTorch model state dictionaries.\"\"\"\n",
        "    try:\n",
        "        save_path = get_root_dir().joinpath(dirname)\n",
        "        if not os.path.isdir(save_path):\n",
        "            os.mkdir(save_path)\n",
        "\n",
        "        id_ = f\"-{id}\" if id else \"\"\n",
        "        for model_name, model in models_dict.items():\n",
        "            torch.save(model.state_dict(), save_path.joinpath(f\"{model_name}{id_}.ckpt\"))\n",
        "    except PermissionError:\n",
        "        dirname = tempfile.gettempdir()\n",
        "        print(\n",
        "            f\"\\nThe trained model is saved in the following temporary dirname due to some permission error: {dirname}.\\n\"\n",
        "        )\n",
        "\n",
        "        id_ = f\"-{id}\" if id else \"\"\n",
        "        for model_name, model in models_dict.items():\n",
        "            torch.save(model.state_dict(), Path(dirname).joinpath(f\"{model_name}{id_}.ckpt\"))\n",
        "\n",
        "\n",
        "def time_to_timefreq(x, n_fft: int, C: int):\n",
        "    \"\"\"Converts time-domain signal to time-frequency representation.\"\"\"\n",
        "    x = rearrange(x, \"b c l -> (b c) l\")\n",
        "    x = torch.stft(x, n_fft, normalized=False, return_complex=False)\n",
        "    x = rearrange(x, \"(b c) n t z -> b (c z) n t \", c=C)\n",
        "    return x\n",
        "\n",
        "\n",
        "def timefreq_to_time(x, n_fft: int, C: int):\n",
        "    \"\"\"Converts time-frequency representation back to time-domain signal.\"\"\"\n",
        "    x = rearrange(x, \"b (c z) n t -> (b c) n t z\", c=C)\n",
        "    x = torch.istft(x, n_fft, normalized=False, return_complex=False)\n",
        "    x = rearrange(x, \"(b c) l -> b c l\", c=C)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_var_loss(z):\n",
        "    \"\"\"Computes variance loss.\"\"\"\n",
        "    return torch.relu(1.0 - torch.sqrt(z.var(dim=0) + 1e-4)).mean()\n",
        "\n",
        "\n",
        "def compute_cov_loss(z):\n",
        "    \"\"\"Computes covariance loss.\"\"\"\n",
        "    norm_z = z - z.mean(dim=0)\n",
        "    norm_z = F.normalize(norm_z, p=2, dim=0)  # (batch * feature); l2-norm\n",
        "    fxf_cov_z = torch.mm(norm_z.T, norm_z)  # (feature * feature)\n",
        "    ind = np.diag_indices(fxf_cov_z.shape[0])\n",
        "    fxf_cov_z[ind[0], ind[1]] = torch.zeros(fxf_cov_z.shape[0]).to(norm_z.device)\n",
        "    cov_loss = (fxf_cov_z**2).mean()\n",
        "    return cov_loss\n",
        "\n",
        "\n",
        "def quantize(z, vq_model, transpose_channel_length_axes=False):\n",
        "    \"\"\"Quantizes input using a VQ model.\"\"\"\n",
        "    input_dim = len(z.shape) - 2\n",
        "    if input_dim == 2:\n",
        "        h, w = z.shape[2:]\n",
        "        z = rearrange(z, \"b c h w -> b (h w) c\")\n",
        "        z_q, indices, vq_loss, perplexity = vq_model(z)\n",
        "        z_q = rearrange(z_q, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "    elif input_dim == 1:\n",
        "        if transpose_channel_length_axes:\n",
        "            z = rearrange(z, \"b c l -> b (l) c\")\n",
        "        z_q, indices, vq_loss, perplexity = vq_model(z)\n",
        "        if transpose_channel_length_axes:\n",
        "            z_q = rearrange(z_q, \"b (l) c -> b c l\")\n",
        "    else:\n",
        "        raise ValueError\n",
        "    return z_q, indices, vq_loss, perplexity\n",
        "\n",
        "\n",
        "def zero_pad_high_freq(xf):\n",
        "    \"\"\"Zero-pads high frequencies in a time-frequency representation.\"\"\"\n",
        "    xf_l = torch.zeros(xf.shape).to(xf.device)\n",
        "    xf_l[:, :, 0, :] = xf[:, :, 0, :]\n",
        "    return xf_l\n",
        "\n",
        "\n",
        "def zero_pad_low_freq(xf):\n",
        "    \"\"\"Zero-pads low frequencies in a time-frequency representation.\"\"\"\n",
        "    xf_h = torch.zeros(xf.shape).to(xf.device)\n",
        "    xf_h[:, :, 1:, :] = xf[:, :, 1:, :]\n",
        "    return xf_h\n",
        "\n",
        "\n",
        "def compute_emb_loss(codebook, x, use_cosine_sim, esm_max_codes):\n",
        "    \"\"\"Computes embedding loss.\"\"\"\n",
        "    embed = codebook.embed\n",
        "    flatten = x.reshape(-1, x.shape[-1])\n",
        "\n",
        "    if use_cosine_sim:\n",
        "        flatten = F.normalize(flatten, p=2, dim=-1)\n",
        "        embed = F.normalize(embed, p=2, dim=-1)\n",
        "\n",
        "    ind = torch.randint(0, embed.shape[0], size=(min(esm_max_codes, embed.shape[0]),))\n",
        "    embed = embed[ind]\n",
        "\n",
        "    cov_embed = torch.cov(embed.t())\n",
        "    cov_x = torch.cov(flatten.t())\n",
        "\n",
        "    mean_embed = torch.mean(embed, dim=0)\n",
        "    mean_x = torch.mean(flatten, dim=0)\n",
        "\n",
        "    esm_loss = F.mse_loss(cov_x.detach(), cov_embed) + F.mse_loss(mean_x.detach(), mean_embed)\n",
        "    return esm_loss\n",
        "\n",
        "\n",
        "def compute_downsample_rate(input_length: int, n_fft: int, downsampled_width: int):\n",
        "    \"\"\"Computes the downsample rate.\"\"\"\n",
        "    return (\n",
        "        round(input_length / (np.log2(n_fft) - 1) / downsampled_width)\n",
        "        if input_length >= downsampled_width\n",
        "        else 1\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_performance(ground_truth, predictions):\n",
        "    \"\"\"Computes performance metrics: Composition Error, Reconstruction RMSE, Reconstruction MAE, and MAPE.\"\"\"\n",
        "    ground_truth = np.asarray(ground_truth)\n",
        "    predictions = np.asarray(predictions)\n",
        "\n",
        "    if ground_truth.shape != predictions.shape:\n",
        "        # Attempt to reshape predictions if they are 1D and ground_truth is 2D with one column\n",
        "        if len(predictions.shape) == 1 and len(ground_truth.shape) == 2 and ground_truth.shape[1] == 1:\n",
        "            predictions = predictions.reshape(-1, 1)\n",
        "        else:\n",
        "            raise ValueError(\"Ground truth and predictions must have the same shape.\")\n",
        "\n",
        "    composition_error = np.mean(np.abs(ground_truth - predictions))\n",
        "    reconstruction_rmse = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
        "    reconstruction_mae = mean_absolute_error(ground_truth, predictions)\n",
        "\n",
        "    # Mean Absolute Percentage Error (MAPE)\n",
        "    # Avoid division by zero for ground truth values\n",
        "    # A small epsilon is added to the denominator to handle zero values\n",
        "    mape = np.mean(np.abs((ground_truth - predictions) / (ground_truth + 1e-8))) * 100\n",
        "\n",
        "    return {\n",
        "        \"composition_error\": composition_error,\n",
        "        \"reconstruction_rmse\": reconstruction_rmse,\n",
        "        \"reconstruction_mae\": reconstruction_mae,\n",
        "        \"mape\": mape,\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_compression_ratio(original_size_bytes, compressed_size_bytes):\n",
        "    \"\"\"Computes the compression ratio.\"\"\"\n",
        "    if compressed_size_bytes <= 0:\n",
        "        print(\"Warning: Compressed size is zero or negative. Cannot compute compression ratio.\")\n",
        "        return None\n",
        "    return original_size_bytes / compressed_size_bytes\n",
        "\n",
        "\n",
        "class TimeVQVAE:\n",
        "    \"\"\"A placeholder class for the TimeVQVAE model.\"\"\"\n",
        "    def __init__(self, num_embeddings=512, embedding_dim=64):\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # Dummy codebook for demonstration\n",
        "        self.codebook = torch.randn(num_embeddings, embedding_dim)\n",
        "\n",
        "    def tokenize(self, data):\n",
        "        \"\"\"Dummy tokenization: simply return random indices.\"\"\"\n",
        "        # Dummy tokenization: simply return random indices\n",
        "        # In a real TimeVQVAE, this would involve encoding and VQ layer\n",
        "        seq_len = data.shape[1] if len(data.shape) > 1 else data.shape[0]\n",
        "        return np.random.randint(0, self.num_embeddings, size=(data.shape[0], seq_len // 4)) # Example token length\n",
        "\n",
        "    def reconstruct(self, tokens):\n",
        "        \"\"\"Dummy reconstruction: map tokens back to random data.\"\"\"\n",
        "        # Dummy reconstruction: map tokens back to random data\n",
        "        # In a real TimeVQVAE, this would involve decoding tokens\n",
        "        batch_size, token_len = tokens.shape\n",
        "        # Assuming original data length was 4 times token length\n",
        "        reconstructed_len = token_len * 4\n",
        "        return np.random.rand(batch_size, reconstructed_len)\n",
        "\n",
        "\n",
        "def your_model_inference_with_compression(data, vqvae_model):\n",
        "    \"\"\"Placeholder for model inference with dummy compression and reconstruction.\"\"\"\n",
        "    # Convert pandas DataFrame to numpy array for processing\n",
        "    numeric_data = data.select_dtypes(include=np.number).values\n",
        "\n",
        "    # Simulate tokenization (compression)\n",
        "    tokens = vqvae_model.tokenize(numeric_data)\n",
        "\n",
        "    # Simulate reconstruction (decompression)\n",
        "    predictions = vqvae_model.reconstruct(tokens)\n",
        "\n",
        "    # Calculate original and compressed sizes\n",
        "    original_size_bytes = numeric_data.nbytes\n",
        "    # Assuming each token is an integer (e.g., 4 bytes per token)\n",
        "    compressed_size_bytes = tokens.nbytes\n",
        "\n",
        "    return predictions, original_size_bytes, compressed_size_bytes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gJoWjgnFcvEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kotneZpPb7xQ",
        "outputId": "be3523e7-c920-4a98-d438-6306c334fd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting TimeVQVAE Experiment ---\n",
            "\n",
            "--- Processing Dataset: ETTh1 ---\n",
            "ETTh1 loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 4.8999\n",
            "  Reconstruction RMSE: 6.5045\n",
            "  Reconstruction MAE: 4.8999\n",
            "  MAPE: 62514976.0000\n",
            "  Compression Ratio: 7.00\n",
            "\n",
            "--- Processing Dataset: ETTh2 ---\n",
            "ETTh2 loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 24.1082\n",
            "  Reconstruction RMSE: 30.3314\n",
            "  Reconstruction MAE: 24.1082\n",
            "  MAPE: 351125824.0000\n",
            "  Compression Ratio: 7.00\n",
            "\n",
            "--- Processing Dataset: ETTm1 ---\n",
            "ETTm1 loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 4.9152\n",
            "  Reconstruction RMSE: 6.5238\n",
            "  Reconstruction MAE: 4.9152\n",
            "  MAPE: 61533812.0000\n",
            "  Compression Ratio: 7.00\n",
            "\n",
            "--- Processing Dataset: ETTm2 ---\n",
            "ETTm2 loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 24.1329\n",
            "  Reconstruction RMSE: 30.3569\n",
            "  Reconstruction MAE: 24.1329\n",
            "  MAPE: 357303328.0000\n",
            "  Compression Ratio: 7.00\n",
            "\n",
            "--- Processing Dataset: Electricity ---\n",
            "Electricity loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 2535.8132\n",
            "  Reconstruction RMSE: 15263.0891\n",
            "  Reconstruction MAE: 2535.8054\n",
            "  MAPE: 54588708.0000\n",
            "  Compression Ratio: 4.01\n",
            "\n",
            "--- Processing Dataset: Traffic ---\n",
            "Traffic loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 0.4495\n",
            "  Reconstruction RMSE: 0.5318\n",
            "  Reconstruction MAE: 0.4495\n",
            "  MAPE: 45053820.0000\n",
            "  Compression Ratio: 4.01\n",
            "\n",
            "--- Processing Dataset: Weather ---\n",
            "Weather loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 177.7841\n",
            "  Reconstruction RMSE: 407.1455\n",
            "  Reconstruction MAE: 177.7840\n",
            "  MAPE: 838204736.0000\n",
            "  Compression Ratio: 4.20\n",
            "\n",
            "--- Processing Dataset: National Illness ---\n",
            "National Illness loaded successfully!\n",
            "Performance Metrics:\n",
            "  Composition Error: 2104.4241\n",
            "  Reconstruction RMSE: 4526.3177\n",
            "  Reconstruction MAE: 2104.4248\n",
            "  MAPE: 82.3582\n",
            "  Compression Ratio: 7.00\n",
            "\n",
            "--- Final Comparative Analysis of Performance Across Datasets ---\n",
            "         Dataset  Composition Error  Reconstruction RMSE Reconstruction MAE         MAPE  Compression Ratio\n",
            "           ETTh1           4.899922             6.504484               None 6.251498e+07           7.000000\n",
            "           ETTh2          24.108244            30.331390               None 3.511258e+08           7.000000\n",
            "           ETTm1           4.915201             6.523769               None 6.153381e+07           7.000000\n",
            "           ETTm2          24.132858            30.356933               None 3.573033e+08           7.000000\n",
            "     Electricity        2535.813232         15263.089071               None 5.458871e+07           4.012500\n",
            "         Traffic           0.449503             0.531809               None 4.505382e+07           4.009302\n",
            "         Weather         177.784134           407.145513               None 8.382047e+08           4.200000\n",
            "National Illness        2104.424072          4526.317709               None 8.235818e+01           7.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1269579.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from time_vqvae_utils import compute_performance, compute_compression_ratio, TimeVQVAE, your_model_inference_with_compression\n",
        "\n",
        "# List of dataset names and paths\n",
        "dataset_info = {\n",
        "    'ETTh1': '/content/ETTh1.csv',\n",
        "    'ETTh2': '/content/ETTh2.csv',\n",
        "    'ETTm1': '/content/ETTm1.csv',\n",
        "    'ETTm2':  '/content/ETTm2.csv',\n",
        "    'Electricity': '/content/electricity.csv',\n",
        "    'Traffic': '/content/traffic.csv',\n",
        "    'Weather':  '/content/weather.csv',\n",
        "    'National Illness': '/content/national_illness.csv'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize TimeVQVAE model (placeholder)\n",
        "vqvae_model = TimeVQVAE(num_embeddings=512, embedding_dim=64)\n",
        "\n",
        "# Dictionaries to store results\n",
        "performance_results = {}\n",
        "compression_results = {}\n",
        "\n",
        "print(\"\\n--- Starting TimeVQVAE Experiment ---\")\n",
        "\n",
        "for dataset_name, dataset_path in dataset_info.items():\n",
        "    print(f\"\\n--- Processing Dataset: {dataset_name} ---\")\n",
        "    try:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        print(f\"{dataset_name} loaded successfully!\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {dataset_name} file not found at {dataset_path}. Skipping.\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading {dataset_name}: {e}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Get predictions and size information from your model\n",
        "        predictions, original_size, compressed_size = your_model_inference_with_compression(df, vqvae_model)\n",
        "\n",
        "        # Ensure ground truth and predictions have the same shape and are numeric for performance metrics\n",
        "        input_data = df.select_dtypes(include=np.number).values\n",
        "        try:\n",
        "            ground_truth = input_data.astype(np.float32)\n",
        "            predictions = predictions.astype(np.float32)\n",
        "\n",
        "            # Adjust predictions shape to match ground_truth if necessary (e.g., if model outputs shorter sequences)\n",
        "            min_rows = min(ground_truth.shape[0], predictions.shape[0])\n",
        "            min_cols = min(ground_truth.shape[1], predictions.shape[1])\n",
        "            ground_truth_subset = ground_truth[:min_rows, :min_cols]\n",
        "            predictions_subset = predictions[:min_rows, :min_cols]\n",
        "\n",
        "            # Compute performance metrics\n",
        "            metrics = compute_performance(ground_truth_subset, predictions_subset)\n",
        "            performance_results[dataset_name] = metrics\n",
        "\n",
        "            print(\"Performance Metrics:\")\n",
        "            print(f\"  Composition Error: {metrics.get('composition_error', 'N/A'):.4f}\")\n",
        "            print(f\"  Reconstruction RMSE: {metrics.get('reconstruction_rmse', 'N/A'):.4f}\")\n",
        "            print(f\"  Reconstruction MAE: {metrics.get('reconstruction_mae', 'N/A'):.4f}\")\n",
        "            print(f\"  MAPE: {metrics.get('mape', 'N/A'):.4f}\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"  Could not compute performance for {dataset_name}: {e}\")\n",
        "            print(\"  Please ensure the output of your model has a compatible shape and data type for comparison with the ground truth.\")\n",
        "            metrics = {} # Store empty metrics if computation fails\n",
        "        except Exception as e:\n",
        "            print(f\"  An unexpected error occurred while computing performance for {dataset_name}: {e}\")\n",
        "            metrics = {} # Store empty metrics if computation fails\n",
        "\n",
        "\n",
        "        # Compute compression ratio\n",
        "        compression_ratio = compute_compression_ratio(original_size, compressed_size)\n",
        "        compression_results[dataset_name] = compression_ratio\n",
        "\n",
        "        if compression_ratio is not None:\n",
        "            print(f\"  Compression Ratio: {compression_ratio:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  An error occurred during model inference or compression for {dataset_name}: {e}\")\n",
        "\n",
        "# Print final summary in a structured table\n",
        "print(\"\\n--- Final Comparative Analysis of Performance Across Datasets ---\")\n",
        "\n",
        "results_df = pd.DataFrame(columns=[\n",
        "    'Dataset',\n",
        "    'Composition Error',\n",
        "    'Reconstruction RMSE',\n",
        "    'Reconstruction MAE',\n",
        "    'MAPE',\n",
        "    'Compression Ratio'\n",
        "])\n",
        "\n",
        "for dataset_name in dataset_info.keys():\n",
        "    metrics = performance_results.get(dataset_name, {})\n",
        "    comp_ratio = compression_results.get(dataset_name)\n",
        "\n",
        "    new_row = {\n",
        "        'Dataset': dataset_name,\n",
        "        'Composition Error': metrics.get('composition_error'),\n",
        "        'Reconstruction RMSE': metrics.get('reconstruction_rmse'),\n",
        "        'Reconstruction MAE': metrics.get('restraction_mae'),\n",
        "        'MAPE': metrics.get('mape'),\n",
        "        'Compression Ratio': comp_ratio\n",
        "    }\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YGK-S5hlcZrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}