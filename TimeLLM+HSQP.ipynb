{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9842,"status":"ok","timestamp":1771239449944,"user":{"displayName":"shamsuddeen Abdullahi","userId":"10016456727542702193"},"user_tz":-480},"id":"l6R8jCbHjCts","outputId":"051034fc-f831-4db7-a094-ee9969b8a7f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Time-LLM' already exists and is not an empty directory.\n","Setup Complete! You can now run the experiment cell.\n"]}],"source":["# Robust Fixed Colab Setup Cell\n","\n","# 1. Setup Environment and Clone Repositories\n","!git clone https://github.com/KimMeen/Time-LLM.git\n","!pip install einops transformers accelerate sentencepiece protobuf tabulate scikit-learn -q\n","\n","# 2. Create Project Structure\n","import os\n","os.makedirs(\"data\", exist_ok=True )\n","\n","# 3. Create HSQP Tokenizer (ROBUST DIMENSION FIX)\n","with open(\"hsqp_tokenizer.py\", \"w\") as f:\n","    f.write(\"\"\"\n","import torch\n","import torch.nn as nn\n","from einops import rearrange\n","\n","class HSQP_Tokenizer(nn.Module):\n","    def __init__(self, seq_len, patch_len, stride, d_model, n_quant_bins=100):\n","        super(HSQP_Tokenizer, self).__init__()\n","        self.seq_len = seq_len\n","        self.patch_len = patch_len\n","        self.stride = stride\n","        self.n_quant_bins = n_quant_bins\n","        self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n","        self.stat_proj = nn.Linear(4, d_model // 4)\n","        self.quant_embed = nn.Embedding(n_quant_bins, d_model // 4)\n","        self.patch_embed = nn.Linear(patch_len, d_model // 2)\n","        self.final_proj = nn.Linear(d_model, d_model)\n","\n","    def quantize(self, x, n_bins=100):\n","        x_min = x.min(dim=-1, keepdim=True)[0]\n","        x_max = x.max(dim=-1, keepdim=True)[0]\n","        x_norm = (x - x_min) / (x_max - x_min + 1e-8)\n","        return torch.clamp((x_norm * (n_bins - 1)).long(), 0, n_bins - 1)\n","\n","    def forward(self, x):\n","        # Ensure x is float right at the start of the method\n","        x = x.float()\n","\n","        B, N, L = x.shape\n","\n","        x = self.padding_patch_layer(x)\n","        patches = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n","        patches = rearrange(patches, 'b n p l -> (b n) p l')\n","\n","        # 1. Statistical Features\n","        m_mean = patches.mean(dim=-1)\n","        m_std = patches.std(dim=-1)\n","        m_min = patches.min(dim=-1)[0]\n","        m_max = patches.max(dim=-1)[0]\n","\n","        stats = torch.stack([m_mean, m_std, m_min, m_max], dim=-1)\n","        stat_embed = self.stat_proj(stats)\n","\n","        # 2. Quantization Features\n","        quant_indices = self.quantize(patches, self.n_quant_bins)\n","        # Convert quant_indices to float before calling torch.mode, then back to long for embedding\n","        quant_indices_float = quant_indices.float()\n","        quant_mode_float_values = torch.mode(quant_indices_float, dim=-1).values\n","        quant_mode = quant_mode_float_values.long()\n","        quant_embed = self.quant_embed(quant_mode)\n","\n","        # 3. Patch Embeddings\n","        patch_embed = self.patch_embed(patches)\n","\n","        combined = torch.cat([stat_embed, quant_embed, patch_embed], dim=-1)\n","\n","        return self.final_proj(combined), N\n","\n","\"\"\"\n",")\n","\n","# 4. Create Modified Model\n","with open(\"timellm_hsqp.py\", \"w\") as f:\n","    f.write(\"\"\"\n","import sys\n","import os\n","sys.path.append(os.path.abspath(\"Time-LLM\"))\n","sys.path.append(os.path.abspath(\"Time-LLM/models\"))\n","sys.path.append(os.path.abspath(\"Time-LLM/layers\"))\n","\n","import torch\n","import torch.nn as nn\n","from models.TimeLLM import Model as TimeLLM_Base\n","from hsqp_tokenizer import HSQP_Tokenizer\n","\n","class TimeLLM_HSQP(TimeLLM_Base):\n","    def __init__(self, configs, patch_len=16, stride=8, use_hsqp=False):\n","        super().__init__(configs, patch_len, stride)\n","        self.use_hsqp = use_hsqp\n","        if self.use_hsqp:\n","            self.hsqp_tokenizer = HSQP_Tokenizer(configs.seq_len, self.patch_len, self.stride, configs.d_model)\n","\n","    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n","        x_enc = self.normalize_layers(x_enc, 'norm')\n","        B, T, N = x_enc.size()\n","        x_enc_reshaped = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n","        min_values, max_values = x_enc_reshaped.min(1)[0], x_enc_reshaped.max(1)[0]\n","        medians = x_enc_reshaped.median(1).values\n","        lags = self.calcute_lags(x_enc_reshaped)\n","        trends = x_enc_reshaped.diff(dim=1).sum(dim=1)\n","        prompt = [f\"<|start_prompt|>Dataset description: {self.description}Task description: forecast the next {self.pred_len} steps; Input statistics: min {min_values[b].item()}, max {max_values[b].item()}, median {medians[b].item()}, trend {'up' if trends[b]>0 else 'down'}, lags {lags[b].tolist()}<|<end_prompt>|>\" for b in range(B*N)]\n","        x_enc = x_enc_reshaped.reshape(B, N, T).permute(0, 2, 1).contiguous()\n","        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids.to(x_enc.device)\n","        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt)\n","        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n","        x_enc = x_enc.permute(0, 2, 1).contiguous()\n","        enc_out, n_vars = self.hsqp_tokenizer(x_enc) if self.use_hsqp else self.patch_embedding(x_enc)\n","        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n","        dec_out = self.llm_model(inputs_embeds=torch.cat([prompt_embeddings, enc_out], 1)).last_hidden_state[:, :, :self.d_ff]\n","        dec_out = torch.reshape(dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1])).permute(0, 1, 3, 2).contiguous()\n","        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:]).permute(0, 2, 1).contiguous()\n","        return self.normalize_layers(dec_out, 'denorm')\n","\"\"\"\n",")\n","\n","# 5. Create Data Utils\n","with open(\"data_utils.py\", \"w\") as f:\n","    f.write(\"\"\"\n","import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, data, seq_len, pred_len):\n","        self.seq_len, self.pred_len, self.data = seq_len, pred_len, data\n","        self.x, self.y = [], []\n","        for i in range(len(data) - seq_len - pred_len + 1):\n","            self.x.append(data[i:i+seq_len])\n","            self.y.append(data[i+seq_len:i+seq_len+pred_len])\n","    def __len__(self): return len(self.x)\n","    def __getitem__(self, i): return torch.tensor(self.x[i]).float(), torch.zeros(self.seq_len, 1), torch.tensor(self.y[i]).float(), torch.zeros(self.pred_len, 1)\n","\n","def load_data(name, path, seq_len, pred_len):\n","    df = pd.read_csv(path)\n","    df_data = df.iloc[:, 1:].values.astype(np.float32)\n","    n_vars = df_data.shape[1]\n","    tr_sz, val_sz = int(len(df_data)*0.7), int(len(df_data)*0.1)\n","    scaler = StandardScaler()\n","    train = scaler.fit_transform(df_data[:tr_sz])\n","    val = scaler.transform(df_data[tr_sz:tr_sz+val_sz])\n","    test = scaler.transform(df_data[tr_sz+val_sz:])\n","    return TimeSeriesDataset(train, seq_len, pred_len), TimeSeriesDataset(val, seq_len, pred_len), TimeSeriesDataset(test, seq_len, pred_len), n_vars, train\n","\"\"\"\n",")\n","\n","# 6. Create Metrics\n","with open(\"metrics.py\", \"w\") as f:\n","    f.write(\"\"\"\n","import numpy as np\n","def compute_metrics(pred, true, hist):\n","    mse = np.mean((pred - true)**2)\n","    mae = np.mean(np.abs(pred - true))\n","    h_rate = np.mean((pred < hist.min() - 0.2*(hist.max()-hist.min())) | (pred > hist.max() + 0.2*(hist.max()-hist.min()))) * 100\n","    var = np.var(pred - true) + 1e-8\n","    perp = np.exp(0.5 * np.log(2 * np.pi * var) + 0.5)\n","    return {\"MSE\": mse, \"MAE\": mae, \"Hallucination\": h_rate, \"Perplexity\": perp}\n","\"\"\"\n",")\n","\n","# 7. Download Datasets\n","#!wget -q https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv -O data/ETTh1.csv\n","#!wget -q https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh2.csv -O data/ETTh2.csv\n","#!wget -q https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm1.csv -O data/ETTm1.csv\n","#!wget -q https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv -O data/ETTm2.csv\n","!wget -q https://huggingface.co/datasets/pkr7098/time-series-forecasting-datasets/resolve/main/weather.csv -O data/weather.csv\n","#!wget -q https://huggingface.co/datasets/pkr7098/time-series-forecasting-datasets/resolve/main/traffic.csv -O data/traffic.csv\n","!wget -q https://huggingface.co/datasets/pkr7098/time-series-forecasting-datasets/resolve/main/national_illness.csv -O data/traffic.csv\n","print(\"Setup Complete! You can now run the experiment cell.\" )\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"uxugNA3teBFE","outputId":"ea91b402-ce06-4f51-f6d6-d404047c4ecd","executionInfo":{"status":"error","timestamp":1771239481155,"user_tz":-480,"elapsed":44,"user":{"displayName":"shamsuddeen Abdullahi","userId":"10016456727542702193"}}},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"partially initialized module 'torch' has no attribute 'jit' (most likely due to a circular import)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2045050879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Updated Colab Experiment Run Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2202\u001b[0m )\n\u001b[1;32m   2203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2205\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nested/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Allowlist these for weights_only load of NJT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_rebuild_njt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNestedTensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_NestedTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nested/_internal/nested_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDispatchKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDispatchKeySet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_expandable_to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested_int\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNestedIntNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeakTensorKeyDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nested/_internal/nested_int.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constant_symnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantIntNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimmutable_collections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m from torch.fx._symbolic_trace import (  # noqa: F401\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mPH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mProxyableClassMeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lazy_graph_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_make_graph_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_PyTreeCodeGen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PyTreeInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/_lazy_graph_module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from torch.fx.graph_module import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0m_format_import_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from .graph import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0m_custom_builtins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0m_is_from_torch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimmutable_collections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimmutable_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_qualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArgument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/node.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fx_map_aggregate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fx_map_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NodeBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from torch.fx.operator_schemas import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mArgsKwargsPair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnormalize_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/operator_schemas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m\"Layout\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m\"number\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;34m\"Future\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;34m\"AnyEnumType\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"QScheme\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqscheme\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'jit' (most likely due to a circular import)"]}],"source":["# Updated Colab Experiment Run Cell\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import sys\n","import os\n","import numpy as np\n","import pandas as pd\n","from tabulate import tabulate\n","import warnings\n","\n","# Ensure the current directory is in sys.path for local imports\n","sys.path.insert(0, \".\")\n","\n","from timellm_hsqp import TimeLLM_HSQP\n","from data_utils import load_data\n","from metrics import compute_metrics\n","\n","warnings.filterwarnings('ignore')\n","\n","class Config:\n","    def __init__(self, **kwargs):\n","        self.task_name = 'long_term_forecast'\n","        self.prompt_domain = False\n","        self.content = \"\"\n","\n","        self.seq_len = 96\n","        self.pred_len = 192\n","\n","        self.patch_len = 16\n","        self.stride = 16\n","\n","        self.d_model = 64\n","        self.d_ff = 256\n","        self.n_heads = 4\n","        self.dropout = 0.1\n","        self.head_dropout = 0.1\n","\n","        self.llm_model = 'GPT2'\n","        self.llm_layers = 1\n","        self.llm_dim = 768\n","\n","        self.enc_in = 7\n","\n","        self.batch_size = 512\n","        self.epochs = 5\n","        self.learning_rate = 1e-4\n","\n","        # Overwrite defaults with any provided keyword arguments\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","\n","# Configuration for the experiments\n","config = Config(\n","    epochs=2,        # Set to 10 or 20 for final results\n","    llm_layers=1,    # Set to 2 or more for better accuracy\n","    batch_size=64\n",")\n","\n","datasets = {\n","    #'ETTh1': 'data/ETTh1.csv', 'ETTh2': 'data/ETTh2.csv',\n","    #'ETTm1': 'data/ETTm1.csv', 'ETTm2': 'data/ETTm2.csv',\n","    'Weather': 'data/weather.csv',\n","    'national_illness': 'data/national_illness.csv'\n","\n","}\n","\n","results = []\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Running experiments on: {device}\")\n","\n","for name, path in datasets.items():\n","    if not os.path.exists(path):\n","        print(f\"Skipping {name}: File not found at {path}\")\n","        continue\n","\n","    print(f\"\\nProcessing {name}...\")\n","    train_set, val_set, test_set, n_vars, hist_data = load_data(name, path, config.seq_len, config.pred_len)\n","\n","    for use_hsqp in [False, True]:\n","        model_name = \"TimeLLM+HSQP\" if use_hsqp else \"TimeLLM\"\n","        print(f\"  Running {model_name}...\")\n","\n","        # Create a new config object for each model and update enc_in\n","        model_config = Config(**vars(config))\n","        model_config.enc_in = n_vars\n","\n","        model = TimeLLM_HSQP(model_config, use_hsqp=use_hsqp).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n","        criterion = torch.nn.MSELoss()\n","\n","        # Simple training loop\n","        model.train()\n","        train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, num_workers=2)\n","        for epoch in range(config.epochs):\n","            for x, xm, y, ym in train_loader:\n","                optimizer.zero_grad()\n","                pred = model(x.to(device), xm.to(device), y.to(device), ym.to(device))\n","                loss = criterion(pred, y.to(device))\n","                loss.backward()\n","                optimizer.step()\n","\n","        # Evaluation\n","        model.eval()\n","        test_loader = DataLoader(test_set, batch_size=config.batch_size, num_workers=2)\n","        preds, trues = [], []\n","        with torch.no_grad():\n","            for x, xm, y, ym in test_loader:\n","                preds.append(model(x.to(device), xm.to(device), y.to(device), ym.to(device)).cpu().numpy())\n","                trues.append(y.numpy())\n","\n","        m = compute_metrics(np.concatenate(preds), np.concatenate(trues), hist_data)\n","        results.append([name, model_name, f\"{m['MSE']:.4f}\", f\"{m['MAE']:.4f}\", f\"{m['Hallucination']:.2f}%\", f\"{m['Perplexity']:.4f}\"])\n","\n","print(\"\\n\" + tabulate(results, headers=[\"Dataset\", \"Model\", \"MSE\", \"MAE\", \"Hallucination\", \"Perplexity\"], tablefmt=\"grid\"))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOQd+aE+jgG8D4YyqU4DIKl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}