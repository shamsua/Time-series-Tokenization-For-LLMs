{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OTZKtRIP0n3",
        "outputId": "4674f821-ab28-4c4f-f028-c2e8a685e914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (175 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcj_cacqPSNd",
        "outputId": "d5462366-6056-4b72-c7b0-8ad1a2ef8f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMI4yM7q7YH"
      },
      "source": [
        "**Hierarchical Symbolic-Quantized Patching (HSQP) Implementation**\n",
        "This script implements the HSQP approach for time-series tokenization,\n",
        "which integrates patching, ABBA symbolic aggregation, and quantization.\n",
        "\n",
        "The pipeline follows these steps:\n",
        "1. Raw Time Series Input\n",
        "2. Initial Patching (Coarse-Graining)\n",
        "3. ABBA Symbolic Aggregation (Pattern Extraction)\n",
        "4. Quantization of ABBA-Derived Features\n",
        "5. LLM Tokenization and Embedding\n",
        "6. (Optional) Inverse Transformation for forecasting/regression\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir HSQP\n",
        "%cd HSQP\n",
        "!mkdir data config library models utils\n",
        "!touch data/__init__.py\n",
        "!touch library/__init__.py\n",
        "!touch config/__init__.py\n",
        "!touch models/__init__.py\n",
        "!touch utils/__init__.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVA1S6ctRwS4",
        "outputId": "e5ea6b76-361c-497f-8e6f-2c9087723d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HSQP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/drive/folders/1jJF4eyd6jOhtDdBxsBqZxADycebXLKi8?usp=sharing -O ./data --folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3ix4PexAXPuk",
        "outputId": "fdf6ea1b-578b-41d2-827a-808cf3f2ff7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1eBBuh0tErhOi8pn8MbZSdZsLheWQlJmF electricity.csv\n",
            "Processing file 10YIsPPBXP67AIYrRV3L5R1zP7qyuSAcu ETTh1.csv\n",
            "Processing file 1bc2152E8Kz7zT1RUJdEnvfYnaeNSu3so ETTh2.csv\n",
            "Processing file 1pM6fCIjuOYs572Rxh17hS-TyyGp1vGjZ ETTm1.csv\n",
            "Processing file 1CRfjDX4Cgb15Y372j--cnjZQ-ONYMRXE ETTm2.csv\n",
            "Processing file 1O22tzJo80P4CdhF3s3OWvjehV1wARRtC national_illness.csv\n",
            "Processing file 1X2ICEGDQRUR0GbiOcCvE80lrDGYRtj3F timeseries.csv\n",
            "Processing file 1VK13chaIDj8bmSajDfmWGECeZBwx3NEt traffic.csv\n",
            "Processing file 1dcK4jg9qpeEYH7383s1YH6RXKK3cFzBX weather.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eBBuh0tErhOi8pn8MbZSdZsLheWQlJmF\n",
            "To: /content/HSQP/data/electricity.csv\n",
            "100% 36.4M/36.4M [00:01<00:00, 22.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10YIsPPBXP67AIYrRV3L5R1zP7qyuSAcu\n",
            "To: /content/HSQP/data/ETTh1.csv\n",
            "100% 2.59M/2.59M [00:00<00:00, 99.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bc2152E8Kz7zT1RUJdEnvfYnaeNSu3so\n",
            "To: /content/HSQP/data/ETTh2.csv\n",
            "100% 2.42M/2.42M [00:00<00:00, 154MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pM6fCIjuOYs572Rxh17hS-TyyGp1vGjZ\n",
            "To: /content/HSQP/data/ETTm1.csv\n",
            "100% 10.4M/10.4M [00:01<00:00, 10.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CRfjDX4Cgb15Y372j--cnjZQ-ONYMRXE\n",
            "To: /content/HSQP/data/ETTm2.csv\n",
            "100% 9.68M/9.68M [00:00<00:00, 21.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O22tzJo80P4CdhF3s3OWvjehV1wARRtC\n",
            "To: /content/HSQP/data/national_illness.csv\n",
            "100% 57.5k/57.5k [00:00<00:00, 91.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X2ICEGDQRUR0GbiOcCvE80lrDGYRtj3F\n",
            "To: /content/HSQP/data/timeseries.csv\n",
            "100% 215k/215k [00:00<00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VK13chaIDj8bmSajDfmWGECeZBwx3NEt\n",
            "To: /content/HSQP/data/traffic.csv\n",
            "100% 104M/104M [00:01<00:00, 75.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dcK4jg9qpeEYH7383s1YH6RXKK3cFzBX\n",
            "To: /content/HSQP/data/weather.csv\n",
            "100% 6.59M/6.59M [00:00<00:00, 18.7MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title requirements.txt\n",
        "%%writefile requirements.txt\n",
        "scikit-learn\n",
        "dtw-python\n",
        "fastdtw\n",
        "torch\n",
        "scikit-learn\n",
        "pandas\n",
        "matplotlib\n",
        "numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-5slvrR6xq",
        "outputId": "0427bf39-bd66-45b6-f624-5f5f62f5db7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFGm8ZaksYyh",
        "outputId": "35859f55-1e69-40d4-b9fc-a3e20cccc7db",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
            "Collecting dtw-python (from -r requirements.txt (line 2))\n",
            "  Downloading dtw_python-1.7.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting fastdtw (from -r requirements.txt (line 3))\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
            "Downloading dtw_python-1.7.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.0/825.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp312-cp312-linux_x86_64.whl size=567859 sha256=ab402eaa04c14b27d7b9dbf79ecbbfaf052e5887ff22793b9f763abf15dc7b47\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/d0/26/b82cb0f49ae73e5e6bba4e8462fff2c9851d7bd2ec64f8891e\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw, dtw-python\n",
            "Successfully installed dtw-python-1.7.4 fastdtw-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title hsqp_utils.py\n",
        "\n",
        "%%writefile utils/hsqp_utils.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# --- Code from hsqp_utils.py ---\n",
        "\n",
        "class HSQPUtils:\n",
        "  @staticmethod\n",
        "  def load_time_series_data(file_path: str) -> np.ndarray:\n",
        "      \"\"\"\n",
        "      Loads time series data from a CSV file.\n",
        "\n",
        "      Args:\n",
        "          file_path: Path to the CSV file.\n",
        "\n",
        "      Returns:\n",
        "          NumPy array of the time series data.\n",
        "      \"\"\"\n",
        "      if not os.path.exists(file_path):\n",
        "          raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
        "      df = pd.read_csv(file_path)\n",
        "\n",
        "      # Explicitly drop the 'date' column if it exists to ensure only numerical columns remain\n",
        "      if 'date' in df.columns:\n",
        "          df = df.drop(columns=['date'])\n",
        "\n",
        "      # Assuming the time series is in the first numerical column after dropping 'date'\n",
        "      numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "      if len(numeric_cols) == 0:\n",
        "          raise ValueError(\"CSV file does not contain any numerical columns for time series data.\")\n",
        "\n",
        "      # Select the first numerical column and explicitly convert to numeric, coercing errors\n",
        "      ts_series = pd.to_numeric(df[numeric_cols[0]], errors='coerce')\n",
        "\n",
        "      # Drop any rows that resulted in NaN after coercion\n",
        "      ts_series.dropna(inplace=True)\n",
        "\n",
        "      # Return the values as a NumPy array\n",
        "      return ts_series.values\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_compression_ratio(original_data_size: int, compressed_data_size: int) -> float:\n",
        "      \"\"\"\n",
        "      Calculates the compression ratio.\n",
        "\n",
        "      Args:\n",
        "          original_data_size: Size of the original data in bytes.\n",
        "          compressed_data_size: Size of the compressed data in bytes.\n",
        "\n",
        "      Returns:\n",
        "          Compression ratio.\n",
        "      \"\"\"\n",
        "      if compressed_data_size == 0:\n",
        "          return float('inf') # Handle cases where compressed data is empty\n",
        "      return original_data_size / compressed_data_size\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_rmse(original_ts: np.ndarray, reconstructed_ts: np.ndarray) -> float:\n",
        "      \"\"\"\n",
        "      Calculates the Root Mean Squared Error (RMSE).\n",
        "\n",
        "      Args:\n",
        "          original_ts: Original time series.\n",
        "          reconstructed_ts: Reconstructed time series.\n",
        "\n",
        "      Returns:\n",
        "          RMSE value.\n",
        "      \"\"\"\n",
        "      # Ensure both arrays have the same length, truncate the longer one if necessary\n",
        "      min_len = min(len(original_ts), len(reconstructed_ts))\n",
        "      return np.sqrt(mean_squared_error(original_ts[:min_len], reconstructed_ts[:min_len]))\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_mae(original_ts: np.ndarray, reconstructed_ts: np.ndarray) -> float:\n",
        "      \"\"\"\n",
        "      Calculates the Mean Absolute Error (MAE).\n",
        "\n",
        "      Args:\n",
        "          original_ts: Original time series.\n",
        "          reconstructed_ts: Reconstructed time series.\n",
        "\n",
        "      Returns:\n",
        "          MAE value.\n",
        "      \"\"\"\n",
        "      # Ensure both arrays have the same length, truncate the longer one if necessary\n",
        "      min_len = min(len(original_ts), len(reconstructed_ts))\n",
        "      return mean_absolute_error(original_ts[:min_len], reconstructed_ts[:min_len])\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_time_series(original_ts: np.ndarray, reconstructed_ts: np.ndarray, title: str = \"Time Series Reconstruction\"):\n",
        "      \"\"\"\n",
        "      Plots the original and reconstructed time series.\n",
        "\n",
        "      Args:\n",
        "          original_ts: Original time series.\n",
        "          reconstructed_ts: Reconstructed time series.\n",
        "          title: Title of the plot.\n",
        "      \"\"\"\n",
        "      plt.figure(figsize=(15, 6))\n",
        "      plt.plot(original_ts, label=\"Original Time Series\", alpha=0.7)\n",
        "      plt.plot(reconstructed_ts, label=\"Reconstructed Time Series\", alpha=0.7, linestyle=\":\")\n",
        "      plt.title(title)\n",
        "      plt.xlabel(\"Time Step\")\n",
        "      plt.ylabel(\"Value\")\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"{title.replace(' ', '_').lower()}.png\")\n",
        "      plt.close()\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_error_series(original_ts: np.ndarray, reconstructed_ts: np.ndarray, title: str = \"Reconstruction Error\"):\n",
        "      \"\"\"\n",
        "      Plots the reconstruction error series.\n",
        "\n",
        "      Args:\n",
        "          original_ts: Original time series.\n",
        "          reconstructed_ts: Reconstructed time series.\n",
        "          title: Title of the plot.\n",
        "      \"\"\"\n",
        "      min_len = min(len(original_ts), len(reconstructed_ts))\n",
        "      error_ts = original_ts[:min_len] - reconstructed_ts[:min_len]\n",
        "\n",
        "      plt.figure(figsize=(15, 4))\n",
        "      plt.plot(error_ts, label=\"Error\", color='red', alpha=0.7)\n",
        "      plt.title(title)\n",
        "      plt.xlabel(\"Time Step\")\n",
        "      plt.ylabel(\"Error\")\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"{title.replace(' ', '_').lower()}.png\")\n",
        "      plt.close()\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_time_series(original, reconstructed, title=\"Time Series Reconstruction\",\n",
        "                        save_path=None, figsize=(12, 4)):\n",
        "        \"\"\"\n",
        "        Plot original vs reconstructed time series.\n",
        "\n",
        "        Args:\n",
        "            original: Original time series\n",
        "            reconstructed: Reconstructed time series\n",
        "            title: Plot title\n",
        "            save_path: Path to save the plot (optional)\n",
        "            figsize: Figure size\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        # Trim to same length\n",
        "        min_len = min(len(original), len(reconstructed))\n",
        "        time_index = np.arange(min_len)\n",
        "\n",
        "        plt.plot(time_index, original[:min_len], label='Original', alpha=0.7, linewidth=1.5)\n",
        "        plt.plot(time_index, reconstructed[:min_len], label='Reconstructed', alpha=0.7, linewidth=1.5)\n",
        "\n",
        "        plt.title(title, fontsize=14)\n",
        "        plt.xlabel(\"Time Index\", fontsize=12)\n",
        "        plt.ylabel(\"Value\", fontsize=12)\n",
        "        plt.legend(fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add statistics text box\n",
        "        stats_text = f\"RMSE: {np.sqrt(np.mean((original[:min_len] - reconstructed[:min_len])**2)):.4f}\\n\"\n",
        "        stats_text += f\"MAE: {np.mean(np.abs(original[:min_len] - reconstructed[:min_len])):.4f}\"\n",
        "\n",
        "        plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
        "                fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "XnEzurtKsG76",
        "outputId": "eb386388-1408-4d09-a8bc-65af7d025189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/hsqp_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title __init__.py\n",
        "\n",
        "%%writefile utils/__init__.py\n",
        "\n",
        "from .hsqp_utils import HSQPUtils\n",
        "\n",
        "__all__ = ['HSQPUtils']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "gT0PvDBn967z",
        "outputId": "303c74dc-fef4-44cc-a69e-3594e6952f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dataset.py\n",
        "\n",
        "%%writefile data/dataset.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "class ABBASymbolicAggregation:\n",
        "    \"\"\"\n",
        "    Implementation of ABBA (Aggregation-Based Amplitude Scaling) for symbolic pattern extraction (Step 3 in HSQP).\n",
        "    This is a simplified implementation based on the fABBA library concepts.\n",
        "    \"\"\"\n",
        "    def __init__(self, tol: float = 0.1, alpha: float = 0.1, sorting: str = '2-norm', scl: float = 1, k: int = 10, seed: int = 42):\n",
        "        \"\"\"\n",
        "        Initialize ABBA parameters.\n",
        "\n",
        "        Args:\n",
        "            tol: Tolerance for compression\n",
        "            alpha: Parameter for digitization\n",
        "            sorting: Method for sorting ('2-norm', 'area', etc.)\n",
        "            scl: Scaling factor\n",
        "            k: Number of symbols/clusters\n",
        "            seed: Random seed for KMeans reproducibility\n",
        "        \"\"\"\n",
        "        self.tol = tol\n",
        "        self.alpha = alpha\n",
        "        self.sorting = sorting\n",
        "        self.scl = scl\n",
        "        self.k = k\n",
        "        self.seed = seed\n",
        "        self.parameters = None\n",
        "        self.kmeans = None\n",
        "\n",
        "    def compress(self, ts: np.ndarray) -> List[Tuple[float, float]]:\n",
        "        \"\"\"\n",
        "        Compress time series into piecewise linear segments (polygonal chain).\n",
        "\n",
        "        Args:\n",
        "            ts: Time series data\n",
        "\n",
        "        Returns:\n",
        "            List of (len, inc) tuples representing the polygonal segments\n",
        "        \"\"\"\n",
        "        # Ensure ts is a 1D array and explicitly convert to float\n",
        "        ts = np.asarray(ts, dtype=np.float64).flatten()\n",
        "        n = len(ts)\n",
        "\n",
        "        pieces = []\n",
        "        start_idx = 0\n",
        "\n",
        "        while start_idx < n - 1:\n",
        "            end_idx = start_idx + 1\n",
        "            while end_idx < n:\n",
        "                if end_idx == start_idx + 1:\n",
        "                    line_segment = np.array([ts[start_idx], ts[end_idx]])\n",
        "                else:\n",
        "                    t = np.linspace(0, 1, end_idx - start_idx + 1)\n",
        "                    line_segment = ts[start_idx] + (ts[end_idx] - ts[start_idx]) * t\n",
        "\n",
        "                if np.max(np.abs(line_segment - ts[start_idx:end_idx+1])) <= self.tol:\n",
        "                    end_idx += 1\n",
        "                else:\n",
        "                    end_idx -= 1\n",
        "                    break\n",
        "\n",
        "            if end_idx >= n:\n",
        "                end_idx = n - 1\n",
        "\n",
        "            length = end_idx - start_idx\n",
        "            increment = ts[end_idx] - ts[start_idx]\n",
        "\n",
        "            pieces.append((length, increment))\n",
        "\n",
        "            start_idx = end_idx\n",
        "\n",
        "        return pieces\n",
        "\n",
        "    def fit_kmeans(self, features: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit KMeans model to the extracted features.\n",
        "\n",
        "        Args:\n",
        "            features: Features (len, inc) from all pieces\n",
        "        \"\"\"\n",
        "        if features.shape[0] < self.k:\n",
        "            print(f\"Warning: Number of samples ({features.shape[0]}) is less than n_clusters ({self.k}). Adjusting n_clusters to {features.shape[0]}.\")\n",
        "            self.k = features.shape[0]\n",
        "            if self.k == 0:\n",
        "                self.kmeans = None\n",
        "                return\n",
        "\n",
        "        if self.kmeans is None:\n",
        "            # Use the seed for reproducible KMeans clustering\n",
        "            self.kmeans = KMeans(n_clusters=self.k, random_state=self.seed, n_init=10)\n",
        "        self.kmeans.fit(features)\n",
        "        self.parameters = {\n",
        "            'centers': self.kmeans.cluster_centers_,\n",
        "            'scl': self.scl,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "\n",
        "    def predict_symbols(self, features: np.ndarray) -> List[str]:\n",
        "        \"\"\"\n",
        "        Predict symbols for given features using the fitted KMeans model.\n",
        "\n",
        "        Args:\n",
        "            features: Features (len, inc) from pieces\n",
        "\n",
        "        Returns:\n",
        "            List of symbols\n",
        "        \"\"\"\n",
        "        if self.kmeans is None:\n",
        "            raise ValueError(\"KMeans model not fitted. Call fit_kmeans() first.\")\n",
        "\n",
        "        if features.shape[0] == 0:\n",
        "            return []\n",
        "\n",
        "        if self.scl != 1:\n",
        "            features = features / self.scl\n",
        "\n",
        "        labels = self.kmeans.predict(features)\n",
        "\n",
        "        symbols = [chr(97 + label) for label in labels]\n",
        "\n",
        "        return symbols\n",
        "\n",
        "    def inverse_transform(self, string: str, initial_value: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert symbolic representation back to time series.\n",
        "\n",
        "        Args:\n",
        "            string: Symbolic representation\n",
        "            initial_value: Initial value of the time series\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed time series\n",
        "        \"\"\"\n",
        "        if self.parameters is None or self.kmeans is None:\n",
        "            raise ValueError(\"ABBA model must be fitted before inverse transform\")\n",
        "\n",
        "        indices = [ord(s) - 97 for s in string]\n",
        "\n",
        "        centers = self.parameters['centers']\n",
        "\n",
        "        if self.scl != 1:\n",
        "            centers = centers * self.scl\n",
        "\n",
        "        pieces = [tuple(centers[idx]) for idx in indices]\n",
        "\n",
        "        ts_recon = [initial_value]\n",
        "        for length, increment in pieces:\n",
        "            length = int(round(length))\n",
        "            if length < 1:\n",
        "                length = 1\n",
        "\n",
        "            if length == 1:\n",
        "                ts_recon.append(ts_recon[-1] + increment)\n",
        "            else:\n",
        "                for i in range(1, length + 1):\n",
        "                    ts_recon.append(ts_recon[-1] + increment / length)\n",
        "\n",
        "        return np.array(ts_recon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "uBgAgffbrfAS",
        "outputId": "4b2fb13e-a8ff-46d5-a3aa-d7d9f5c68f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data/__init__.py\n",
        "\n",
        "from .dataset import ABBASymbolicAggregation\n",
        "\n",
        "__all__ = [\n",
        "    'ABBASymbolicAggregation',\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k-IEIPq_943",
        "outputId": "ba20651e-d50c-4a6f-94d8-999d8cfc1bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title time_series.py\n",
        "\n",
        "%%writefile library/time_series.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "class TimeSeriesPatching:\n",
        "    \"\"\"\n",
        "    Class for creating and merging patches from time series data (Step 2 in HSQP).\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_length: int = 60, stride: int = 32, overlap: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the patching parameters.\n",
        "\n",
        "        Args:\n",
        "            patch_length: Length of each patch\n",
        "            stride: Step size between patches (if overlap=True)\n",
        "            overlap: Whether patches should overlap\n",
        "        \"\"\"\n",
        "        self.patch_length = patch_length\n",
        "        self.stride = stride if overlap else patch_length\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def create_patches(self, time_series: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create patches from a time series.\n",
        "\n",
        "        Args:\n",
        "            time_series: Time series data of shape [batch_size, seq_length, features]\n",
        "                         or [seq_length, features] or [seq_length]\n",
        "\n",
        "        Returns:\n",
        "            Patches of shape [batch_size, num_patches, patch_length, features]\n",
        "                         or [num_patches, patch_length, features]\n",
        "                         or [num_patches, patch_length]\n",
        "        \"\"\"\n",
        "        original_shape = time_series.shape\n",
        "        if len(original_shape) == 1:\n",
        "            time_series = time_series.reshape(-1, 1)\n",
        "            seq_length, features = time_series.shape\n",
        "            batch_size = None\n",
        "        elif len(original_shape) == 2:\n",
        "            seq_length, features = time_series.shape\n",
        "            batch_size = None\n",
        "        else:\n",
        "            batch_size, seq_length, features = time_series.shape\n",
        "\n",
        "        num_patches = (seq_length - self.patch_length) // self.stride + 1\n",
        "\n",
        "        if num_patches <= 0:\n",
        "            raise ValueError(f\"Number of patches ({num_patches}) is not positive. Ensure seq_length ({seq_length}) is greater than or equal to patch_length ({self.patch_length}) and stride ({self.stride}) allows at least one patch.\")\n",
        "\n",
        "        if batch_size is None:\n",
        "            # Vectorized patch creation\n",
        "            indices = np.arange(num_patches) * self.stride\n",
        "            patches = np.array([time_series[i:i + self.patch_length] for i in indices])\n",
        "            if len(original_shape) == 1:\n",
        "                patches = patches.reshape(num_patches, self.patch_length)\n",
        "        else:\n",
        "            # Batched patch creation\n",
        "            patches = np.zeros((batch_size, num_patches, self.patch_length, features))\n",
        "            for b in range(batch_size):\n",
        "                indices = np.arange(num_patches) * self.stride\n",
        "                patches[b] = np.array([time_series[b, i:i + self.patch_length] for i in indices])\n",
        "\n",
        "        return patches\n",
        "\n",
        "    def merge_patches(self, patches: np.ndarray, original_length: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Merge patches back into a time series.\n",
        "        For overlapping regions, values are averaged.\n",
        "\n",
        "        Args:\n",
        "            patches: Patches of shape [batch_size, num_patches, patch_length, features]\n",
        "                     or [num_patches, patch_length, features]\n",
        "                     or [num_patches, patch_length]\n",
        "            original_length: Original sequence length (optional)\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed time series\n",
        "        \"\"\"\n",
        "        original_patch_shape = patches.shape\n",
        "        if len(original_patch_shape) == 2:\n",
        "            patches = patches.reshape(original_patch_shape[0], original_patch_shape[1], 1)\n",
        "            num_patches, patch_length, features = patches.shape\n",
        "            batch_size = None\n",
        "        elif len(original_patch_shape) == 3:\n",
        "            num_patches, patch_length, features = patches.shape\n",
        "            batch_size = None\n",
        "        else:\n",
        "            batch_size, num_patches, patch_length, features = patches.shape\n",
        "\n",
        "        if original_length is None:\n",
        "            seq_length = (num_patches - 1) * self.stride + patch_length\n",
        "        else:\n",
        "            seq_length = original_length\n",
        "\n",
        "        if batch_size is None:\n",
        "            reconstructed = np.zeros((seq_length, features))\n",
        "            counts = np.zeros((seq_length, features))\n",
        "\n",
        "            for i in range(num_patches):\n",
        "                start_idx = i * self.stride\n",
        "                end_idx = start_idx + patch_length\n",
        "                reconstructed[start_idx:end_idx] += patches[i]\n",
        "                counts[start_idx:end_idx] += 1\n",
        "\n",
        "            reconstructed = reconstructed / np.maximum(counts, 1)\n",
        "\n",
        "            if len(original_patch_shape) == 2:\n",
        "                reconstructed = reconstructed.reshape(seq_length)\n",
        "        else:\n",
        "            reconstructed = np.zeros((batch_size, seq_length, features))\n",
        "            counts = np.zeros((batch_size, seq_length, features))\n",
        "\n",
        "            for b in range(batch_size):\n",
        "                for i in range(num_patches):\n",
        "                    start_idx = i * self.stride\n",
        "                    end_idx = start_idx + patch_length\n",
        "                    reconstructed[b, start_idx:end_idx] += patches[b, i]\n",
        "                    counts[b, i] += 1\n",
        "\n",
        "            reconstructed = reconstructed / np.maximum(counts, 1)\n",
        "\n",
        "        return reconstructed\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "U7lRzkU_rxwX",
        "outputId": "7671d171-8b88-414e-8c91-5047fcc2303a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing library/time_series.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title feature_quantization.py\n",
        "\n",
        "%%writefile library/feature_quantization.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "class FeatureQuantization:\n",
        "    \"\"\"\n",
        "    Quantization of ABBA-derived features for efficiency optimization (Step 4 in HSQP).\n",
        "    \"\"\"\n",
        "    def __init__(self, bit_width: int = 8, method: str = 'affine', block_size: int = 32):\n",
        "        \"\"\"\n",
        "        Initialize quantization parameters.\n",
        "\n",
        "        Args:\n",
        "            bit_width: Target bit width (e.g., 8 for INT8, 4 for INT4)\n",
        "            method: Quantization method (\\'affine\\', \\'abs_max\\')\n",
        "            block_size: Block size for block-wise quantization\n",
        "        \"\"\"\n",
        "        self.bit_width = bit_width\n",
        "        self.method = method\n",
        "        self.block_size = block_size\n",
        "        self.scale = None\n",
        "        self.zero_point = None\n",
        "\n",
        "        self.qmin = -(2 ** (bit_width - 1))\n",
        "        self.qmax = 2 ** (bit_width - 1) - 1\n",
        "\n",
        "    def quantize(self, features: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Quantize features to lower precision.\n",
        "\n",
        "        Args:\n",
        "            features: Input features\n",
        "\n",
        "        Returns:\n",
        "            Quantized features\n",
        "        \"\"\"\n",
        "        if features.size == 0:\n",
        "            return np.array([], dtype=np.int8 if self.bit_width <= 8 else np.int16)\n",
        "\n",
        "        if self.method == 'abs_max':\n",
        "            abs_max = np.max(np.abs(features))\n",
        "            if abs_max == 0:\n",
        "                abs_max = 1.0\n",
        "\n",
        "            self.scale = self.qmax / abs_max\n",
        "            self.zero_point = 0\n",
        "\n",
        "            q_features = np.round(features * self.scale)\n",
        "            q_features = np.clip(q_features, self.qmin, self.qmax)\n",
        "\n",
        "        elif self.method == 'affine':\n",
        "            f_min = np.min(features)\n",
        "            f_max = np.max(features)\n",
        "\n",
        "            if f_min == f_max:\n",
        "                self.scale = 1.0\n",
        "                self.zero_point = 0\n",
        "            else:\n",
        "                self.scale = (self.qmax - self.qmin) / (f_max - f_min)\n",
        "                self.zero_point = self.qmin - round(f_min * self.scale)\n",
        "\n",
        "            q_features = np.round(features * self.scale + self.zero_point)\n",
        "            q_features = np.clip(q_features, self.qmin, self.qmax)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown quantization method: {self.method}\")\n",
        "\n",
        "        return q_features.astype(np.int8 if self.bit_width <= 8 else np.int16)\n",
        "\n",
        "    def dequantize(self, q_features: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Dequantize features back to original precision.\n",
        "\n",
        "        Args:\n",
        "            q_features: Quantized features\n",
        "\n",
        "        Returns:\n",
        "            Dequantized features\n",
        "        \"\"\"\n",
        "        if self.scale is None or (self.method == 'affine' and self.zero_point is None):\n",
        "            raise ValueError(\"Quantization parameters not set. Call quantize() first.\")\n",
        "\n",
        "        if self.method == 'abs_max':\n",
        "            return q_features / self.scale\n",
        "        elif self.method == 'affine':\n",
        "            return (q_features - self.zero_point) / self.scale\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown quantization method: {self.method}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "LHOoQfvWr3_d",
        "outputId": "6f5bad92-c075-45dd-9cb5-9b252918adaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing library/feature_quantization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile library/__init__.py\n",
        "\n",
        "from .feature_quantization import FeatureQuantization\n",
        "from .time_series import TimeSeriesPatching\n",
        "\n",
        "__all__ = [\n",
        "    'FeatureQuantization',\n",
        "    'TimeSeriesPatching',\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJD39y4c0qBw",
        "outputId": "00e2dcae-39ab-4e25-cbd9-d50dfa8ddcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting library/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models/__init__.py\n",
        "\n",
        "from .hsqp import HSQP\n",
        "\n",
        "__all__ = ['HSQP']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlEGZvedwiPN",
        "outputId": "0b90b37d-a923-414c-eb04-e137c745b7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b358b67",
        "outputId": "8c0a92f7-c715-40ff-fdf0-977f0df29127",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models/hsqp.py\n"
          ]
        }
      ],
      "source": [
        "#@title hsqp.py\n",
        "\n",
        "%%writefile models/hsqp.py\n",
        "# --- Updated models/hsqp.py ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from library import FeatureQuantization, TimeSeriesPatching\n",
        "from data import ABBASymbolicAggregation\n",
        "\n",
        "class HSQP:\n",
        "    \"\"\"\n",
        "    Hierarchical Symbolic-Quantized Patching (HSQP) with Ablation support.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 patch_length: int = 60,\n",
        "                 stride: int = 22,\n",
        "                 tol: float = 0.02,\n",
        "                 alpha: float = 0.1,\n",
        "                 k: int = 26,\n",
        "                 bit_width: int = 8,\n",
        "                 quant_method: str = 'affine',\n",
        "                 embedding_dim: int = 128,\n",
        "                 use_patching: bool = True,\n",
        "                 use_abba: bool = True,\n",
        "                 use_quant: bool = True,\n",
        "                 seed: int = 42):\n",
        "\n",
        "        # Store seed\n",
        "        self.seed = seed\n",
        "\n",
        "        # Ablation Flags\n",
        "        self.use_patching = use_patching\n",
        "        self.use_abba = use_abba\n",
        "        self.use_quant = use_quant\n",
        "\n",
        "        # Modules (pass seed to components that need it)\n",
        "        self.patching = TimeSeriesPatching(patch_length=patch_length, stride=stride)\n",
        "        self.abba = ABBASymbolicAggregation(tol=tol, alpha=alpha, k=k, seed=seed)\n",
        "        self.quantization = FeatureQuantization(bit_width=bit_width, method=quant_method)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = None\n",
        "\n",
        "    def fit_transform(self, time_series: np.ndarray) -> Tuple[List[str], np.ndarray, List[List[Tuple[float, float]]]]:\n",
        "        # 1. Patching Ablation\n",
        "        if self.use_patching:\n",
        "            patches = self.patching.create_patches(time_series)\n",
        "        else:\n",
        "            patches = time_series.reshape(1, -1)\n",
        "\n",
        "        # ... [num_patches calculation remains the same] ...\n",
        "        num_patches = patches.shape[1] if len(patches.shape) == 4 else patches.shape[0]\n",
        "        is_batched = len(patches.shape) == 4\n",
        "\n",
        "        all_pieces = []\n",
        "        pieces_list = []\n",
        "\n",
        "        for i in range(num_patches):\n",
        "            patch = patches[0, i].flatten() if is_batched else patches[i].flatten()\n",
        "\n",
        "            if self.use_abba:\n",
        "                pieces = self.abba.compress(patch)\n",
        "            else:\n",
        "                # Bypass ABBA: Use raw (value, time_index) as \"pieces\"\n",
        "                pieces = [(val, float(idx)) for idx, val in enumerate(patch)]\n",
        "\n",
        "            pieces_list.append(pieces)\n",
        "            all_pieces.extend(pieces)\n",
        "\n",
        "        all_features = np.array(all_pieces)\n",
        "\n",
        "        # 3. Quantization Ablation\n",
        "        if self.use_quant and all_features.size > 0:\n",
        "            quantized_features = self.quantization.quantize(all_features)\n",
        "        else:\n",
        "            quantized_features = all_features\n",
        "\n",
        "        # FIX: Store the results in the object so inverse_transform can see them\n",
        "        self.quantized_cache = quantized_features\n",
        "        self.symbols_cache = pieces_list # Useful for non-ABBA reconstruction\n",
        "\n",
        "        # Fit K-Means only if using ABBA\n",
        "        symbols_list = []\n",
        "        if self.use_abba and all_features.shape[0] > 0:\n",
        "            self.abba.fit_kmeans(all_features)\n",
        "            for pieces in pieces_list:\n",
        "                symbols = self.abba.predict_symbols(np.array(pieces))\n",
        "                symbols_list.append(''.join(symbols))\n",
        "        else:\n",
        "            symbols_list = [\"NOSYM\"] * len(pieces_list)\n",
        "\n",
        "        return symbols_list, quantized_features, pieces_list\n",
        "\n",
        "    def inverse_transform(self, symbols_list: List[str], initial_values: List[float], original_length: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert symbolic representation back to time series with ablation handling.\n",
        "        \"\"\"\n",
        "        # If ABBA was bypassed, we reconstruct directly from the cached features\n",
        "        if not self.use_abba:\n",
        "            features = self.quantization.dequantize(self.quantized_cache) if self.use_quant else self.quantized_cache\n",
        "            # features is (N, 2) where col 0 is value. Extract values.\n",
        "            return features[:, 0].flatten()[:original_length]\n",
        "\n",
        "        # Standard ABBA Reconstruction logic\n",
        "        reconstructed_patches = []\n",
        "        for symbol, initial_value in zip(symbols_list, initial_values):\n",
        "            if symbol == \"NOSYM\": continue\n",
        "            patch_recon = self.abba.inverse_transform(symbol, initial_value)\n",
        "            reconstructed_patches.append(patch_recon)\n",
        "\n",
        "        if not reconstructed_patches:\n",
        "            return np.array([])\n",
        "\n",
        "        if not self.use_patching:\n",
        "            return np.concatenate(reconstructed_patches)[:original_length]\n",
        "\n",
        "        # Standard Patch Merge\n",
        "        target_patch_length = self.patching.patch_length\n",
        "        uniform_patches = [np.pad(p, (0, max(0, target_patch_length - len(p))), 'edge')[:target_patch_length] for p in reconstructed_patches]\n",
        "        return self.patching.merge_patches(np.array(uniform_patches), original_length)\n",
        "\n",
        "    def create_llm_embeddings(self, quantized_features: np.ndarray) -> np.ndarray:\n",
        "        if quantized_features.size == 0: return np.array([])\n",
        "\n",
        "        input_dim = quantized_features.shape[1] if len(quantized_features.shape) > 1 else 1\n",
        "        if self.embedding is None:\n",
        "            # Set PyTorch seed for embedding initialization\n",
        "            torch.manual_seed(self.seed)\n",
        "            self.embedding = nn.Linear(input_dim, self.embedding_dim)\n",
        "\n",
        "        q_features_tensor = torch.tensor(quantized_features, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            return self.embedding(q_features_tensor).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title main.py\n",
        "\n",
        "%%writefile main.py\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import math\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from models import HSQP\n",
        "from utils import HSQPUtils\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Set random seed for reproducibility across NumPy, PyTorch, and Python's random module.\n",
        "\n",
        "    Args:\n",
        "        seed: Integer seed value\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # For deterministic behavior in PyTorch (may impact performance)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"Random seed set to: {seed}\")\n",
        "\n",
        "def get_dataset_params(dataset_name):\n",
        "    \"\"\"\n",
        "    Returns specific configuration based on the dataset type.\n",
        "    \"\"\"\n",
        "    configs = {\n",
        "        'timeseries': {\n",
        "            'file': 'data/timeseries.csv',\n",
        "            'patch_length': 60,\n",
        "            'stride': 32,\n",
        "            'k': 26\n",
        "        },\n",
        "        'electricity': {\n",
        "            'file': 'data/electricity.csv',\n",
        "            'patch_length': 60,\n",
        "            'stride': 32,\n",
        "            'k': 26\n",
        "        },\n",
        "        'national_illness': {\n",
        "            'file': 'data/national_illness.csv',\n",
        "            'patch_length': 96,\n",
        "            'stride': 48,\n",
        "            'k': 30\n",
        "        },\n",
        "        'etth1': {\n",
        "            'file': 'data/ETTh1.csv',\n",
        "            'patch_length': 144,\n",
        "            'stride': 72,\n",
        "            'k': 20\n",
        "        },\n",
        "        'etth2': {\n",
        "            'file': 'data/ETTh2.csv',\n",
        "            'patch_length': 144,\n",
        "            'stride': 72,\n",
        "            'k': 20\n",
        "        },\n",
        "        'ettm1': {\n",
        "            'file': 'data/ETTm1.csv',\n",
        "            'patch_length': 144,\n",
        "            'stride': 72,\n",
        "            'k': 20\n",
        "        },\n",
        "        'ettm2': {\n",
        "            'file': 'data/ETTm2.csv',\n",
        "            'patch_length': 144,\n",
        "            'stride': 72,\n",
        "            'k': 20\n",
        "        }\n",
        "    }\n",
        "    return configs.get(dataset_name.lower(), configs['electricity'])\n",
        "\n",
        "class TokenEntropyAnalyzer:\n",
        "    \"\"\"Analyzer for token/symbol entropy in HSQP.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_symbol_entropy(symbols_list):\n",
        "        \"\"\"\n",
        "        Calculate entropy of symbols generated by ABBA.\n",
        "\n",
        "        Args:\n",
        "            symbols_list: List of symbol strings\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with various entropy metrics\n",
        "        \"\"\"\n",
        "        if not symbols_list:\n",
        "            return {\n",
        "                'symbol_entropy': np.nan,\n",
        "                'symbol_uniqueness': np.nan,\n",
        "                'symbol_distribution': None,\n",
        "                'effective_alphabet_size': np.nan\n",
        "            }\n",
        "\n",
        "        # Flatten all symbols into a single sequence\n",
        "        all_symbols = ''.join(symbols_list)\n",
        "\n",
        "        if not all_symbols:\n",
        "            return {\n",
        "                'symbol_entropy': np.nan,\n",
        "                'symbol_uniqueness': np.nan,\n",
        "                'symbol_distribution': None,\n",
        "                'effective_alphabet_size': np.nan\n",
        "            }\n",
        "\n",
        "        # Calculate symbol frequencies\n",
        "        symbol_counts = Counter(all_symbols)\n",
        "        total_symbols = len(all_symbols)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        probs = np.array([count / total_symbols for count in symbol_counts.values()])\n",
        "\n",
        "        # Calculate Shannon entropy\n",
        "        shannon_entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "        # Maximum possible entropy (uniform distribution)\n",
        "        max_entropy = np.log2(len(symbol_counts)) if len(symbol_counts) > 0 else 0\n",
        "\n",
        "        # Normalized entropy (0 to 1)\n",
        "        normalized_entropy = shannon_entropy / max_entropy if max_entropy > 0 else 0\n",
        "\n",
        "        # Symbol uniqueness ratio\n",
        "        unique_symbols = len(symbol_counts)\n",
        "        total_occurrences = total_symbols\n",
        "        uniqueness_ratio = unique_symbols / total_occurrences if total_occurrences > 0 else 0\n",
        "\n",
        "        # Effective alphabet size (2^H)\n",
        "        effective_alphabet_size = 2 ** shannon_entropy if not np.isnan(shannon_entropy) else 0\n",
        "\n",
        "        # Symbol distribution\n",
        "        symbol_distribution = {\n",
        "            'unique_symbols': unique_symbols,\n",
        "            'total_symbols': total_symbols,\n",
        "            'most_common': symbol_counts.most_common(10),  # Top 10 symbols\n",
        "            'symbol_frequencies': {k: v/total_symbols for k, v in symbol_counts.items()}\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'symbol_entropy': shannon_entropy,\n",
        "            'symbol_entropy_normalized': normalized_entropy,\n",
        "            'symbol_uniqueness': uniqueness_ratio,\n",
        "            'symbol_distribution': symbol_distribution,\n",
        "            'effective_alphabet_size': effective_alphabet_size,\n",
        "            'max_possible_entropy': max_entropy,\n",
        "            'unique_symbol_count': unique_symbols,\n",
        "            'total_symbol_count': total_symbols\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_n_gram_entropy(symbols_list, n=2):\n",
        "        \"\"\"\n",
        "        Calculate n-gram entropy for symbols.\n",
        "\n",
        "        Args:\n",
        "            symbols_list: List of symbol strings\n",
        "            n: n-gram size (default: 2 for bigrams)\n",
        "\n",
        "        Returns:\n",
        "            n-gram entropy\n",
        "        \"\"\"\n",
        "        if not symbols_list:\n",
        "            return np.nan\n",
        "\n",
        "        all_symbols = ''.join(symbols_list)\n",
        "\n",
        "        if len(all_symbols) < n:\n",
        "            return np.nan\n",
        "\n",
        "        # Extract n-grams\n",
        "        n_grams = [all_symbols[i:i+n] for i in range(len(all_symbols) - n + 1)]\n",
        "\n",
        "        # Calculate n-gram frequencies\n",
        "        n_gram_counts = Counter(n_grams)\n",
        "        total_n_grams = len(n_grams)\n",
        "\n",
        "        # Calculate probabilities and entropy\n",
        "        probs = np.array([count / total_n_grams for count in n_gram_counts.values()])\n",
        "        n_gram_entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "        return n_gram_entropy\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_quantization_entropy(quantized_features):\n",
        "        \"\"\"\n",
        "        Calculate entropy of quantized features.\n",
        "\n",
        "        Args:\n",
        "            quantized_features: Tensor of quantized features\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with quantization entropy metrics\n",
        "        \"\"\"\n",
        "        if quantized_features is None:\n",
        "            return {\n",
        "                'quant_entropy': np.nan,\n",
        "                'quant_value_distribution': None,\n",
        "                'quant_sparsity': np.nan\n",
        "            }\n",
        "\n",
        "        # Flatten the features\n",
        "        features_flat = quantized_features.flatten().numpy() if torch.is_tensor(quantized_features) else quantized_features.flatten()\n",
        "\n",
        "        # Discretize for entropy calculation (bin into 256 levels)\n",
        "        if len(features_flat) > 0:\n",
        "            # Normalize to 0-255 range\n",
        "            min_val = np.min(features_flat)\n",
        "            max_val = np.max(features_flat)\n",
        "\n",
        "            if max_val > min_val:\n",
        "                normalized = (features_flat - min_val) / (max_val - min_val)\n",
        "                discretized = (normalized * 255).astype(int)\n",
        "            else:\n",
        "                discretized = np.zeros_like(features_flat, dtype=int)\n",
        "\n",
        "            # Calculate entropy\n",
        "            value_counts = Counter(discretized)\n",
        "            total_values = len(discretized)\n",
        "            probs = np.array([count / total_values for count in value_counts.values()])\n",
        "            quant_entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "            # Calculate sparsity (percentage of zero or near-zero values)\n",
        "            sparsity = np.sum(np.abs(features_flat) < 1e-6) / len(features_flat)\n",
        "\n",
        "            # Value distribution statistics\n",
        "            value_distribution = {\n",
        "                'unique_values': len(value_counts),\n",
        "                'min_value': float(np.min(features_flat)),\n",
        "                'max_value': float(np.max(features_flat)),\n",
        "                'mean_value': float(np.mean(features_flat)),\n",
        "                'std_value': float(np.std(features_flat))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            quant_entropy = np.nan\n",
        "            sparsity = np.nan\n",
        "            value_distribution = None\n",
        "\n",
        "        return {\n",
        "            'quant_entropy': quant_entropy,\n",
        "            'quant_value_distribution': value_distribution,\n",
        "            'quant_sparsity': sparsity\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_joint_entropy(symbols_list, quantized_features):\n",
        "        \"\"\"\n",
        "        Calculate joint entropy between symbols and quantized features.\n",
        "\n",
        "        Args:\n",
        "            symbols_list: List of symbol strings\n",
        "            quantized_features: Tensor of quantized features\n",
        "\n",
        "        Returns:\n",
        "            Joint entropy and mutual information metrics\n",
        "        \"\"\"\n",
        "        if not symbols_list or quantized_features is None:\n",
        "            return {\n",
        "                'joint_entropy': np.nan,\n",
        "                'mutual_information': np.nan,\n",
        "                'normalized_mutual_information': np.nan\n",
        "            }\n",
        "\n",
        "        # Get symbol entropy\n",
        "        symbol_metrics = TokenEntropyAnalyzer.calculate_symbol_entropy(symbols_list)\n",
        "        symbol_entropy = symbol_metrics['symbol_entropy']\n",
        "\n",
        "        # Get quantization entropy\n",
        "        quant_metrics = TokenEntropyAnalyzer.calculate_quantization_entropy(quantized_features)\n",
        "        quant_entropy = quant_metrics['quant_entropy']\n",
        "\n",
        "        # For joint entropy, we'd need joint distribution\n",
        "        # This is a simplified version - in practice you'd need the joint probability distribution\n",
        "        if not np.isnan(symbol_entropy) and not np.isnan(quant_entropy):\n",
        "            # Upper bound for joint entropy (sum of individual entropies)\n",
        "            joint_entropy_upper = symbol_entropy + quant_entropy\n",
        "\n",
        "            # Simplified mutual information approximation\n",
        "            # In practice, you'd calculate this from joint distribution\n",
        "            mutual_info = max(0, symbol_entropy + quant_entropy - joint_entropy_upper)\n",
        "\n",
        "            # Normalized mutual information\n",
        "            normalized_mi = mutual_info / min(symbol_entropy, quant_entropy) if min(symbol_entropy, quant_entropy) > 0 else np.nan\n",
        "        else:\n",
        "            joint_entropy_upper = np.nan\n",
        "            mutual_info = np.nan\n",
        "            normalized_mi = np.nan\n",
        "\n",
        "        return {\n",
        "            'joint_entropy_upper_bound': joint_entropy_upper,\n",
        "            'mutual_information_approx': mutual_info,\n",
        "            'normalized_mutual_information': normalized_mi\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def print_entropy_report(entropy_metrics, dataset_name):\n",
        "        \"\"\"Print comprehensive entropy analysis report.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"TOKEN ENTROPY ANALYSIS - {dataset_name.upper()}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Symbol Entropy\n",
        "        if 'symbol_entropy' in entropy_metrics:\n",
        "            print(f\"\\n SYMBOL ENTROPY:\")\n",
        "            print(f\"  Shannon Entropy: {entropy_metrics.get('symbol_entropy', np.nan):.4f} bits\")\n",
        "            print(f\"  Normalized Entropy: {entropy_metrics.get('symbol_entropy_normalized', np.nan):.4f} (0-1 scale)\")\n",
        "            print(f\"  Max Possible Entropy: {entropy_metrics.get('max_possible_entropy', np.nan):.4f} bits\")\n",
        "            print(f\"  Effective Alphabet Size: {entropy_metrics.get('effective_alphabet_size', np.nan):.2f}\")\n",
        "            print(f\"  Unique Symbols: {entropy_metrics.get('unique_symbol_count', 0)}\")\n",
        "            print(f\"  Total Symbols: {entropy_metrics.get('total_symbol_count', 0)}\")\n",
        "            print(f\"  Symbol Uniqueness Ratio: {entropy_metrics.get('symbol_uniqueness', np.nan):.4f}\")\n",
        "\n",
        "            # N-gram entropies\n",
        "            for n in [2, 3]:\n",
        "                ngram_key = f'{n}_gram_entropy'\n",
        "                if ngram_key in entropy_metrics:\n",
        "                    print(f\"  {n}-gram Entropy: {entropy_metrics[ngram_key]:.4f} bits\")\n",
        "\n",
        "        # Quantization Entropy\n",
        "        if 'quant_entropy' in entropy_metrics:\n",
        "            print(f\"\\n QUANTIZATION ENTROPY:\")\n",
        "            print(f\"  Quantization Entropy: {entropy_metrics.get('quant_entropy', np.nan):.4f} bits\")\n",
        "            print(f\"  Quantization Sparsity: {entropy_metrics.get('quant_sparsity', np.nan):.4%}\")\n",
        "\n",
        "            dist = entropy_metrics.get('quant_value_distribution')\n",
        "            if dist:\n",
        "                print(f\"  Unique Quantized Values: {dist.get('unique_values', 0)}\")\n",
        "                print(f\"  Value Range: [{dist.get('min_value', 0):.4f}, {dist.get('max_value', 0):.4f}]\")\n",
        "                print(f\"  Mean ± Std: {dist.get('mean_value', 0):.4f} ± {dist.get('std_value', 0):.4f}\")\n",
        "\n",
        "        # Joint Entropy\n",
        "        if 'joint_entropy_upper_bound' in entropy_metrics:\n",
        "            print(f\"\\n JOINT ENTROPY & MUTUAL INFORMATION:\")\n",
        "            print(f\"  Joint Entropy Upper Bound: {entropy_metrics.get('joint_entropy_upper_bound', np.nan):.4f} bits\")\n",
        "            print(f\"  Mutual Information (approx): {entropy_metrics.get('mutual_information_approx', np.nan):.4f} bits\")\n",
        "            print(f\"  Normalized Mutual Information: {entropy_metrics.get('normalized_mutual_information', np.nan):.4f}\")\n",
        "\n",
        "        print(f\"\\n ENTROPY INTERPRETATION:\")\n",
        "        symbol_entropy = entropy_metrics.get('symbol_entropy', np.nan)\n",
        "        if not np.isnan(symbol_entropy):\n",
        "            if symbol_entropy < 2:\n",
        "                print(f\"  Symbol Entropy ({symbol_entropy:.2f} bits): LOW - Highly repetitive patterns\")\n",
        "            elif symbol_entropy < 4:\n",
        "                print(f\"  Symbol Entropy ({symbol_entropy:.2f} bits): MODERATE - Some structure present\")\n",
        "            else:\n",
        "                print(f\"  Symbol Entropy ({symbol_entropy:.2f} bits): HIGH - Complex, information-rich\")\n",
        "\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "class MetricsCalculator:\n",
        "    \"\"\"Comprehensive metrics calculator for time series reconstruction.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_mse(original, reconstructed):\n",
        "        \"\"\"Calculate Mean Squared Error.\"\"\"\n",
        "        if len(original) != len(reconstructed):\n",
        "            min_len = min(len(original), len(reconstructed))\n",
        "            original = original[:min_len]\n",
        "            reconstructed = reconstructed[:min_len]\n",
        "\n",
        "        if len(original) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        return np.mean((original - reconstructed) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_mae(original, reconstructed):\n",
        "        \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
        "        if len(original) != len(reconstructed):\n",
        "            min_len = min(len(original), len(reconstructed))\n",
        "            original = original[:min_len]\n",
        "            reconstructed = reconstructed[:min_len]\n",
        "\n",
        "        if len(original) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        return np.mean(np.abs(original - reconstructed))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_rmse(original, reconstructed):\n",
        "        \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
        "        mse = MetricsCalculator.calculate_mse(original, reconstructed)\n",
        "        return np.sqrt(mse) if not np.isnan(mse) else np.nan\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_mape(original, reconstructed, epsilon=1e-10):\n",
        "        \"\"\"\n",
        "        Calculate Mean Absolute Percentage Error.\n",
        "\n",
        "        Args:\n",
        "            original: Original time series\n",
        "            reconstructed: Reconstructed time series\n",
        "            epsilon: Small value to avoid division by zero\n",
        "        \"\"\"\n",
        "        if len(original) != len(reconstructed):\n",
        "            min_len = min(len(original), len(reconstructed))\n",
        "            original = original[:min_len]\n",
        "            reconstructed = reconstructed[:min_len]\n",
        "\n",
        "        if len(original) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        # Avoid division by zero\n",
        "        denominator = np.abs(original)\n",
        "        denominator[denominator < epsilon] = epsilon\n",
        "\n",
        "        mape = np.mean(np.abs((original - reconstructed) / denominator)) * 100\n",
        "        return mape\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_rrmse(original, reconstructed):\n",
        "        \"\"\"Calculate Relative Root Mean Squared Error.\"\"\"\n",
        "        rmse = MetricsCalculator.calculate_rmse(original, reconstructed)\n",
        "        if np.isnan(rmse) or np.std(original) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        return rmse / np.std(original)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_rmae(original, reconstructed):\n",
        "        \"\"\"Calculate Relative Mean Absolute Error.\"\"\"\n",
        "        mae = MetricsCalculator.calculate_mae(original, reconstructed)\n",
        "        if np.isnan(mae) or np.mean(np.abs(original)) == 0:\n",
        "            return np.nan\n",
        "\n",
        "        return mae / np.mean(np.abs(original))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_compression_ratio(original_ts, symbols_list, quantized_features,\n",
        "                                   use_abba=True, use_quant=True):\n",
        "        \"\"\"\n",
        "        Calculate compression ratio.\n",
        "\n",
        "        Args:\n",
        "            original_ts: Original time series\n",
        "            symbols_list: List of symbols from ABBA\n",
        "            quantized_features: Quantized features tensor\n",
        "            use_abba: Whether ABBA was used\n",
        "            use_quant: Whether quantization was used\n",
        "        \"\"\"\n",
        "        # Original size in bytes\n",
        "        original_size = original_ts.nbytes\n",
        "\n",
        "        # Calculate compressed size\n",
        "        compressed_size = 0\n",
        "\n",
        "        if use_abba and symbols_list:\n",
        "            # Estimate symbol storage size (1 byte per symbol if using ASCII)\n",
        "            symbols_size = sum(len(s) for s in symbols_list)\n",
        "            compressed_size += symbols_size\n",
        "\n",
        "        if use_quant and quantized_features is not None:\n",
        "            # Quantized features size\n",
        "            if torch.is_tensor(quantized_features):\n",
        "                compressed_size += quantized_features.nelement() * quantized_features.element_size()\n",
        "            else:\n",
        "                compressed_size += quantized_features.nbytes\n",
        "\n",
        "        # If nothing was compressed, ratio is 1\n",
        "        if compressed_size == 0:\n",
        "            return 1.0\n",
        "\n",
        "        return original_size / compressed_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_all_metrics(original, reconstructed, symbols_list=None,\n",
        "                             quantized_features=None, use_abba=True, use_quant=True):\n",
        "        \"\"\"\n",
        "        Calculate all comprehensive metrics including token entropy.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all metrics\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Error metrics\n",
        "        metrics['MSE'] = MetricsCalculator.calculate_mse(original, reconstructed)\n",
        "        metrics['MAE'] = MetricsCalculator.calculate_mae(original, reconstructed)\n",
        "        metrics['RMSE'] = MetricsCalculator.calculate_rmse(original, reconstructed)\n",
        "        metrics['MAPE'] = MetricsCalculator.calculate_mape(original, reconstructed)\n",
        "        metrics['RRMSE'] = MetricsCalculator.calculate_rrmse(original, reconstructed)\n",
        "        metrics['RMAE'] = MetricsCalculator.calculate_rmae(original, reconstructed)\n",
        "\n",
        "        # Compression metrics\n",
        "        metrics['Compression_Ratio'] = MetricsCalculator.calculate_compression_ratio(\n",
        "            original, symbols_list, quantized_features, use_abba, use_quant\n",
        "        )\n",
        "\n",
        "        # Token Entropy metrics\n",
        "        entropy_metrics = {}\n",
        "        if symbols_list:\n",
        "            symbol_entropy = TokenEntropyAnalyzer.calculate_symbol_entropy(symbols_list)\n",
        "            entropy_metrics.update(symbol_entropy)\n",
        "\n",
        "            # Calculate n-gram entropies\n",
        "            for n in [2, 3]:\n",
        "                ngram_entropy = TokenEntropyAnalyzer.calculate_n_gram_entropy(symbols_list, n)\n",
        "                entropy_metrics[f'{n}_gram_entropy'] = ngram_entropy\n",
        "\n",
        "        if quantized_features is not None:\n",
        "            quant_entropy = TokenEntropyAnalyzer.calculate_quantization_entropy(quantized_features)\n",
        "            entropy_metrics.update(quant_entropy)\n",
        "\n",
        "        if symbols_list and quantized_features is not None:\n",
        "            joint_entropy = TokenEntropyAnalyzer.calculate_joint_entropy(symbols_list, quantized_features)\n",
        "            entropy_metrics.update(joint_entropy)\n",
        "\n",
        "        # Add entropy metrics to main metrics\n",
        "        metrics.update(entropy_metrics)\n",
        "\n",
        "        # Additional statistics\n",
        "        metrics['Original_Length'] = len(original)\n",
        "        metrics['Reconstructed_Length'] = len(reconstructed)\n",
        "        metrics['Num_Symbols'] = sum(len(s) for s in symbols_list) if symbols_list else 0\n",
        "        metrics['Quantized_Features_Shape'] = quantized_features.shape if quantized_features is not None else None\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def print_metrics_table(metrics, dataset_name, ablation_tag=\"\"):\n",
        "        \"\"\"Print metrics in a formatted table.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"COMPREHENSIVE METRICS REPORT\")\n",
        "        print(f\"Dataset: {dataset_name.upper()} {ablation_tag}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        print(f\"{'Metric':<25} {'Value':<20} {'Unit/Description':<35}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Error metrics (with 4 decimal places)\n",
        "        error_metrics = ['MSE', 'MAE', 'RMSE', 'MAPE', 'RRMSE', 'RMAE']\n",
        "        for metric in error_metrics:\n",
        "            value = metrics.get(metric, np.nan)\n",
        "            if not np.isnan(value):\n",
        "                if metric == 'MAPE':\n",
        "                    print(f\"{metric:<25} {value:.4f}%{'':<12} Percentage error\")\n",
        "                else:\n",
        "                    print(f\"{metric:<25} {value:.6f}{'':<12} Absolute error\")\n",
        "\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Compression metric\n",
        "        comp_ratio = metrics.get('Compression_Ratio', 1.0)\n",
        "        print(f\"{'Compression_Ratio':<25} {comp_ratio:.2f}x{'':<12} Size reduction factor\")\n",
        "\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Token Entropy metrics\n",
        "        entropy_metrics = [\n",
        "            'symbol_entropy', 'symbol_entropy_normalized',\n",
        "            'quant_entropy', 'effective_alphabet_size'\n",
        "        ]\n",
        "\n",
        "        print(f\"{'TOKEN ENTROPY METRICS':<25}\")\n",
        "        print(f\"{'-'*25}\")\n",
        "\n",
        "        if 'symbol_entropy' in metrics:\n",
        "            print(f\"{'Symbol_Entropy':<25} {metrics['symbol_entropy']:.4f}{'':<12} bits\")\n",
        "\n",
        "        if 'symbol_entropy_normalized' in metrics:\n",
        "            print(f\"{'Symbol_Entropy_Norm':<25} {metrics['symbol_entropy_normalized']:.4f}{'':<12} (0-1 scale)\")\n",
        "\n",
        "        if 'quant_entropy' in metrics:\n",
        "            print(f\"{'Quantization_Entropy':<25} {metrics['quant_entropy']:.4f}{'':<12} bits\")\n",
        "\n",
        "        if 'effective_alphabet_size' in metrics:\n",
        "            print(f\"{'Effective_Alphabet_Size':<25} {metrics['effective_alphabet_size']:.2f}{'':<12} symbols\")\n",
        "\n",
        "        # N-gram entropies\n",
        "        for n in [2, 3]:\n",
        "            ngram_key = f'{n}_gram_entropy'\n",
        "            if ngram_key in metrics and not np.isnan(metrics[ngram_key]):\n",
        "                print(f\"{f'{n}-Gram_Entropy':<25} {metrics[ngram_key]:.4f}{'':<12} bits\")\n",
        "\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Additional info\n",
        "        print(f\"{'Original_Length':<25} {metrics.get('Original_Length', 0):<20} Data points\")\n",
        "        print(f\"{'Reconstructed_Length':<25} {metrics.get('Reconstructed_Length', 0):<20} Data points\")\n",
        "        print(f\"{'Num_Symbols':<25} {metrics.get('Num_Symbols', 0):<20} Total symbols\")\n",
        "\n",
        "        if metrics.get('unique_symbol_count'):\n",
        "            print(f\"{'Unique_Symbols':<25} {metrics['unique_symbol_count']:<20} Distinct symbols\")\n",
        "\n",
        "        if metrics.get('Quantized_Features_Shape'):\n",
        "            shape_str = str(metrics['Quantized_Features_Shape'])\n",
        "            print(f\"{'Quantized_Shape':<25} {shape_str:<20} (patches × features)\")\n",
        "\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def save_metrics_to_csv(metrics, dataset_name, ablation_tag, output_dir=\"results\"):\n",
        "        \"\"\"Save metrics to CSV file.\"\"\"\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        csv_file = output_path / f\"metrics_{dataset_name}_{ablation_tag}.csv\"\n",
        "\n",
        "        # Prepare data for CSV\n",
        "        metrics_data = {\n",
        "            'Dataset': [dataset_name],\n",
        "            'Ablation_Tag': [ablation_tag],\n",
        "            'Timestamp': [pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')]\n",
        "        }\n",
        "\n",
        "        # Add all metrics\n",
        "        for key, value in metrics.items():\n",
        "            if isinstance(value, (list, tuple, dict)):\n",
        "                metrics_data[key] = [str(value)]\n",
        "            elif isinstance(value, np.ndarray):\n",
        "                metrics_data[key] = [str(value.shape)]\n",
        "            elif torch.is_tensor(value):\n",
        "                metrics_data[key] = [str(value.shape)]\n",
        "            else:\n",
        "                metrics_data[key] = [value]\n",
        "\n",
        "        # Create DataFrame and save\n",
        "        df = pd.DataFrame(metrics_data)\n",
        "\n",
        "        # Append to existing file or create new\n",
        "        if csv_file.exists():\n",
        "            existing_df = pd.read_csv(csv_file)\n",
        "            df = pd.concat([existing_df, df], ignore_index=True)\n",
        "\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"Metrics saved to: {csv_file}\")\n",
        "\n",
        "        # Also save a summary CSV with key metrics only\n",
        "        summary_file = output_path / f\"summary_{dataset_name}.csv\"\n",
        "        key_metrics = [\n",
        "            'Dataset', 'Ablation_Tag', 'Timestamp',\n",
        "            'MSE', 'MAE', 'RMSE', 'MAPE', 'Compression_Ratio',\n",
        "            'symbol_entropy', 'symbol_entropy_normalized',\n",
        "            'effective_alphabet_size', 'unique_symbol_count'\n",
        "        ]\n",
        "\n",
        "        summary_data = {k: metrics_data.get(k, [np.nan]) for k in key_metrics}\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "        if summary_file.exists():\n",
        "            existing_summary = pd.read_csv(summary_file)\n",
        "            summary_df = pd.concat([existing_summary, summary_df], ignore_index=True)\n",
        "\n",
        "        summary_df.to_csv(summary_file, index=False)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"HSQP Multi-Dataset & Ablation Runner with Comprehensive Metrics\")\n",
        "\n",
        "    # Seed for reproducibility\n",
        "    parser.add_argument('--seed', type=int, default=42,\n",
        "                        help=\"Random seed for reproducibility (default: 42)\")\n",
        "\n",
        "    # Output control\n",
        "    parser.add_argument('--output_dir', type=str, default='results',\n",
        "                        help=\"Directory to save metrics and plots (default: 'results')\")\n",
        "    parser.add_argument('--save_metrics', action='store_true',\n",
        "                        help=\"Save metrics to CSV file\")\n",
        "    parser.add_argument('--no_plots', action='store_true',\n",
        "                        help=\"Disable plotting\")\n",
        "    parser.add_argument('--entropy_analysis', action='store_true',\n",
        "                        help=\"Enable detailed entropy analysis\")\n",
        "\n",
        "    # Dataset Selection\n",
        "    parser.add_argument('--dataset', type=str, default='timeseries',\n",
        "                        choices=['electricity', 'timeseries', 'national_illness', 'etth1', 'etth2', 'ettm1', 'ettm2'],\n",
        "                        help=\"Select predefined dataset or 'custom'\")\n",
        "    parser.add_argument('--data_path', type=str, help=\"Path to custom CSV file\")\n",
        "\n",
        "    # Model Hyperparameters (Defaults will be overridden by dataset choice if not provided)\n",
        "    parser.add_argument('--patch_length', type=int)\n",
        "    parser.add_argument('--stride', type=int)\n",
        "    parser.add_argument('--k', type=int, help=\"Number of symbols\")\n",
        "\n",
        "    # Ablation Flags\n",
        "    parser.add_argument('--no_patching', action='store_true')\n",
        "    parser.add_argument('--no_abba', action='store_true')\n",
        "    parser.add_argument('--no_quant', action='store_true')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # --- Load Dataset Configuration ---\n",
        "    ds_config = get_dataset_params(args.dataset)\n",
        "\n",
        "    # Use command line override if provided, else use dataset default\n",
        "    data_file = args.data_path if args.data_path else ds_config['file']\n",
        "    p_len = args.patch_length if args.patch_length else ds_config['patch_length']\n",
        "    stride = args.stride if args.stride else ds_config['stride']\n",
        "    k_val = args.k if args.k else ds_config['k']\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"HSQP EXPERIMENT: {args.dataset.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  Dataset: {args.dataset}\")\n",
        "    print(f\"  Patch Length: {p_len}\")\n",
        "    print(f\"  Stride: {stride}\")\n",
        "    print(f\"  K (symbols): {k_val}\")\n",
        "    print(f\"  Patching: {'Disabled' if args.no_patching else 'Enabled'}\")\n",
        "    print(f\"  ABBA: {'Disabled' if args.no_abba else 'Enabled'}\")\n",
        "    print(f\"  Quantization: {'Disabled' if args.no_quant else 'Enabled'}\")\n",
        "    print(f\"  Seed: {args.seed}\")\n",
        "    print(f\"  Entropy Analysis: {'Enabled' if args.entropy_analysis else 'Disabled'}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # --- Load Data ---\n",
        "    try:\n",
        "        original_ts = HSQPUtils.load_time_series_data(data_file)\n",
        "        print(f\"Loaded time series: {len(original_ts)} points from {data_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Could not load {data_file}. {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Initialize HSQP Model ---\n",
        "    hsqp = HSQP(\n",
        "        patch_length=p_len,\n",
        "        stride=stride,\n",
        "        k=k_val,\n",
        "        use_patching=not args.no_patching,\n",
        "        use_abba=not args.no_abba,\n",
        "        use_quant=not args.no_quant,\n",
        "        seed=args.seed  # Pass seed to HSQP model\n",
        "    )\n",
        "\n",
        "    # --- Apply HSQP Transformation ---\n",
        "    print(\"\\nApplying HSQP transformation...\")\n",
        "    try:\n",
        "        symbols_list, quantized_features, pieces_list = hsqp.fit_transform(original_ts)\n",
        "        print(f\"✓ Transformation successful\")\n",
        "        print(f\"  Quantized Features Shape: {quantized_features.shape}\")\n",
        "        print(f\"  Number of symbol sequences: {len(symbols_list)}\")\n",
        "\n",
        "        # Print symbol statistics\n",
        "        if symbols_list:\n",
        "            total_symbols = sum(len(s) for s in symbols_list)\n",
        "            unique_symbols = len(set(''.join(symbols_list)))\n",
        "            print(f\"  Total symbols generated: {total_symbols}\")\n",
        "            print(f\"  Unique symbols: {unique_symbols}\")\n",
        "            print(f\"  Symbol diversity: {unique_symbols/total_symbols:.2%}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Transformation failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Reconstruction ---\n",
        "    print(\"\\nReconstructing time series...\")\n",
        "    try:\n",
        "        if not args.no_patching:\n",
        "            patches = hsqp.patching.create_patches(original_ts)\n",
        "            initial_values = [p[0, 0] if len(p.shape) > 1 else p[0] for p in patches]\n",
        "        else:\n",
        "            initial_values = [original_ts[0]]\n",
        "\n",
        "        reconstructed_ts = hsqp.inverse_transform(symbols_list, initial_values, len(original_ts))\n",
        "        print(f\"✓ Reconstruction successful: {len(reconstructed_ts)} points\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Reconstruction failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Calculate Comprehensive Metrics ---\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CALCULATING METRICS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Ensure same length for metrics calculation\n",
        "    min_len = min(len(original_ts), len(reconstructed_ts))\n",
        "    original_for_metrics = original_ts[:min_len]\n",
        "    reconstructed_for_metrics = reconstructed_ts[:min_len]\n",
        "\n",
        "    # Calculate all metrics\n",
        "    all_metrics = MetricsCalculator.calculate_all_metrics(\n",
        "        original=original_for_metrics,\n",
        "        reconstructed=reconstructed_for_metrics,\n",
        "        symbols_list=symbols_list,\n",
        "        quantized_features=quantized_features,\n",
        "        use_abba=not args.no_abba,\n",
        "        use_quant=not args.no_quant\n",
        "    )\n",
        "\n",
        "    # Create ablation tag for identification\n",
        "    ablation_tag = f\"p{int(not args.no_patching)}_a{int(not args.no_abba)}_q{int(not args.no_quant)}\"\n",
        "\n",
        "    # Print metrics table\n",
        "    MetricsCalculator.print_metrics_table(all_metrics, args.dataset, ablation_tag)\n",
        "\n",
        "    # Detailed entropy analysis if requested\n",
        "    if args.entropy_analysis and symbols_list:\n",
        "        TokenEntropyAnalyzer.print_entropy_report(all_metrics, args.dataset)\n",
        "\n",
        "    # Save metrics to CSV if requested\n",
        "    if args.save_metrics:\n",
        "        MetricsCalculator.save_metrics_to_csv(\n",
        "            all_metrics, args.dataset, ablation_tag, args.output_dir\n",
        "        )\n",
        "\n",
        "    # --- Generate Summary Statistics ---\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"Original Time Series:\")\n",
        "    print(f\"  Length: {len(original_ts)}\")\n",
        "    print(f\"  Mean: {np.mean(original_ts):.4f}\")\n",
        "    print(f\"  Std: {np.std(original_ts):.4f}\")\n",
        "    print(f\"  Min: {np.min(original_ts):.4f}\")\n",
        "    print(f\"  Max: {np.max(original_ts):.4f}\")\n",
        "\n",
        "    print(f\"\\nReconstructed Time Series:\")\n",
        "    print(f\"  Length: {len(reconstructed_ts)}\")\n",
        "    print(f\"  Mean: {np.mean(reconstructed_ts):.4f}\")\n",
        "    print(f\"  Std: {np.std(reconstructed_ts):.4f}\")\n",
        "    print(f\"  Min: {np.min(reconstructed_ts):.4f}\")\n",
        "    print(f\"  Max: {np.max(reconstructed_ts):.4f}\")\n",
        "\n",
        "    # --- Key Performance Indicators with Entropy ---\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"KEY PERFORMANCE INDICATORS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    compression_ratio = all_metrics.get('Compression_Ratio', 1.0)\n",
        "    rmse = all_metrics.get('RMSE', np.nan)\n",
        "    mape = all_metrics.get('MAPE', np.nan)\n",
        "    symbol_entropy = all_metrics.get('symbol_entropy', np.nan)\n",
        "    effective_alphabet = all_metrics.get('effective_alphabet_size', np.nan)\n",
        "\n",
        "    print(f\"Compression Efficiency: {compression_ratio:.2f}x size reduction\")\n",
        "    print(f\"Reconstruction Accuracy: RMSE = {rmse:.4f}\")\n",
        "    print(f\"Relative Error: MAPE = {mape:.2f}%\")\n",
        "\n",
        "    if not np.isnan(symbol_entropy):\n",
        "        print(f\"Information Content: Symbol Entropy = {symbol_entropy:.2f} bits\")\n",
        "        print(f\"Effective Alphabet Size: {effective_alphabet:.1f} symbols\")\n",
        "\n",
        "    # Qualitative assessment\n",
        "    if not np.isnan(rmse):\n",
        "        if rmse < 0.01:\n",
        "            print(f\"Quality: Excellent reconstruction (RMSE < 0.01)\")\n",
        "        elif rmse < 0.1:\n",
        "            print(f\"Quality: Good reconstruction (RMSE < 0.1)\")\n",
        "        elif rmse < 0.5:\n",
        "            print(f\"Quality: Acceptable reconstruction (RMSE < 0.5)\")\n",
        "        else:\n",
        "            print(f\"Quality: Poor reconstruction (RMSE ≥ 0.5)\")\n",
        "\n",
        "    # Entropy interpretation\n",
        "    if not np.isnan(symbol_entropy):\n",
        "        if symbol_entropy < 2:\n",
        "            print(f\"Symbol Complexity: Low entropy (repetitive patterns)\")\n",
        "        elif symbol_entropy < 3:\n",
        "            print(f\"Symbol Complexity: Moderate entropy (structured)\")\n",
        "        else:\n",
        "            print(f\"Symbol Complexity: High entropy (complex patterns)\")\n",
        "\n",
        "    # --- Plotting ---\n",
        "    if not args.no_plots:\n",
        "        print(f\"\\nGenerating plots...\")\n",
        "        plot_tag = f\"{args.dataset}_{ablation_tag}\"\n",
        "\n",
        "        # Plot 1: Original vs Reconstructed\n",
        "        HSQPUtils.plot_time_series(\n",
        "            original_ts,\n",
        "            reconstructed_ts[:len(original_ts)],\n",
        "            f\"Reconstruction - {args.dataset.upper()} ({ablation_tag})\",\n",
        "            save_path=output_dir / f\"reconstruction_{plot_tag}.png\"\n",
        "        )\n",
        "\n",
        "        # Plot 2: Error distribution\n",
        "        errors = original_for_metrics - reconstructed_for_metrics\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "        # Error histogram\n",
        "        axes[0, 0].hist(errors, bins=50, alpha=0.7, edgecolor='black', density=True)\n",
        "        axes[0, 0].set_title(f\"Error Distribution - {args.dataset}\")\n",
        "        axes[0, 0].set_xlabel(\"Reconstruction Error\")\n",
        "        axes[0, 0].set_ylabel(\"Density\")\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Error sequence\n",
        "        axes[0, 1].plot(errors[:min(500, len(errors))], alpha=0.7)\n",
        "        axes[0, 1].set_title(f\"Error Sequence (first 500 points)\")\n",
        "        axes[0, 1].set_xlabel(\"Time Index\")\n",
        "        axes[0, 1].set_ylabel(\"Error\")\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Symbol distribution (if available)\n",
        "        if symbols_list and 'symbol_distribution' in all_metrics:\n",
        "            symbol_dist = all_metrics['symbol_distribution']\n",
        "            if symbol_dist and 'most_common' in symbol_dist:\n",
        "                symbols, counts = zip(*symbol_dist['most_common'])\n",
        "                axes[1, 0].bar(range(len(symbols)), counts, alpha=0.7)\n",
        "                axes[1, 0].set_title(f\"Top {len(symbols)} Most Common Symbols\")\n",
        "                axes[1, 0].set_xlabel(\"Symbol\")\n",
        "                axes[1, 0].set_ylabel(\"Frequency\")\n",
        "                axes[1, 0].set_xticks(range(len(symbols)))\n",
        "                axes[1, 0].set_xticklabels(symbols, rotation=45)\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Entropy metrics visualization\n",
        "        axes[1, 1].axis('off')\n",
        "        if not np.isnan(symbol_entropy):\n",
        "            metrics_text = \"Entropy Metrics:\\n\"\n",
        "            metrics_text += f\"Symbol Entropy: {symbol_entropy:.2f} bits\\n\"\n",
        "            metrics_text += f\"Normalized: {all_metrics.get('symbol_entropy_normalized', 0):.2f}\\n\"\n",
        "            metrics_text += f\"Effective Alphabet: {effective_alphabet:.1f}\\n\"\n",
        "            metrics_text += f\"Unique Symbols: {all_metrics.get('unique_symbol_count', 0)}\\n\"\n",
        "\n",
        "            if 'quant_entropy' in all_metrics:\n",
        "                metrics_text += f\"\\nQuant Entropy: {all_metrics['quant_entropy']:.2f} bits\"\n",
        "\n",
        "            axes[1, 1].text(0.1, 0.5, metrics_text,\n",
        "                          fontsize=10, verticalalignment='center',\n",
        "                          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "        plt.suptitle(f\"HSQP Analysis - {args.dataset.upper()} ({ablation_tag})\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_dir / f\"analysis_{plot_tag}.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Plots saved to: {output_dir}/\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXPERIMENT COMPLETE\")\n",
        "    print(f\"Ablation Tag: {ablation_tag}\")\n",
        "    print(f\"Output Directory: {output_dir}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R87vS5u9tGNN",
        "outputId": "4cc70167-ecb6-4a1a-d589-cf8437f10cb3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msiGvb6s3Bz5",
        "outputId": "34d9b731-9c55-4a39-b494-71e3fe449632",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: TIMESERIES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: timeseries\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Enabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Enabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Disabled\n",
            "================================================================================\n",
            "Loaded time series: 7306 points from data/timeseries.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (13112, 2)\n",
            "  Number of symbol sequences: 227\n",
            "  Total symbols generated: 13112\n",
            "  Unique symbols: 26\n",
            "  Symbol diversity: 0.20%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 7306 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: TIMESERIES p1_a1_q1\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       0.288126             Absolute error\n",
            "MAE                       0.380585             Absolute error\n",
            "RMSE                      0.536774             Absolute error\n",
            "MAPE                      10.4584%             Percentage error\n",
            "RRMSE                     0.276446             Absolute error\n",
            "RMAE                      0.086325             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         1.49x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            4.1049             bits\n",
            "Symbol_Entropy_Norm       0.8733             (0-1 scale)\n",
            "Quantization_Entropy      4.4427             bits\n",
            "Effective_Alphabet_Size   17.21             symbols\n",
            "2-Gram_Entropy            8.1372             bits\n",
            "3-Gram_Entropy            11.4467             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           7306                 Data points\n",
            "Reconstructed_Length      7306                 Data points\n",
            "Num_Symbols               13112                Total symbols\n",
            "Unique_Symbols            26                   Distinct symbols\n",
            "Quantized_Shape           (13112, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.4088\n",
            "  Std: 1.9417\n",
            "  Min: 0.6800\n",
            "  Max: 11.9000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.3974\n",
            "  Std: 2.0406\n",
            "  Min: -0.1377\n",
            "  Max: 12.5494\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 1.49x size reduction\n",
            "Reconstruction Accuracy: RMSE = 0.5368\n",
            "Relative Error: MAPE = 10.46%\n",
            "Information Content: Symbol Entropy = 4.10 bits\n",
            "Effective Alphabet Size: 17.2 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: High entropy (complex patterns)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a1_q1\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --no_patching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKtBCNCoJqaL",
        "outputId": "842060d5-7623-4008-a027-4bfc32abfc90",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: TIMESERIES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: timeseries\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Disabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Enabled\n",
            "  Seed: 42\n",
            "================================================================================\n",
            "Loaded time series: 7306 points from data/timeseries.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (7149, 2)\n",
            "  Number of symbol sequences: 1\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 7295 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: TIMESERIES p0_a1_q1\n",
            "================================================================================\n",
            "Metric               Value                Unit/Description                        \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                  16.615156                Absolute error\n",
            "MAE                  3.293676                Absolute error\n",
            "RMSE                 4.076169                Absolute error\n",
            "MAPE                 104.9411%                Percentage error\n",
            "RRMSE                2.098894                Absolute error\n",
            "RMAE                 0.746716                Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio    2.72x                Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length      7295                 Data points\n",
            "Reconstructed_Length 7295                 Data points\n",
            "Num_Symbols          7149                 Total symbols\n",
            "Quantized_Shape      (7149, 2)            (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.4088\n",
            "  Std: 1.9417\n",
            "  Min: 0.6800\n",
            "  Max: 11.9000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 7295\n",
            "  Mean: 7.2430\n",
            "  Std: 2.9022\n",
            "  Min: -0.9479\n",
            "  Max: 15.8458\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 2.72x size reduction\n",
            "Reconstruction Accuracy: RMSE = 4.0762\n",
            "Relative Error: MAPE = 104.94%\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1000x400)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p0_a1_q1\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --no_abba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lKZBtWnJz9M",
        "outputId": "e61312c7-7667-4548-ef9a-66e2bf6437bb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: TIMESERIES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: timeseries\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Enabled\n",
            "  ABBA: Disabled\n",
            "  Quantization: Enabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Disabled\n",
            "================================================================================\n",
            "Loaded time series: 7306 points from data/timeseries.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (13620, 2)\n",
            "  Number of symbol sequences: 227\n",
            "  Total symbols generated: 1135\n",
            "  Unique symbols: 5\n",
            "  Symbol diversity: 0.44%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 7306 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: TIMESERIES p1_a0_q1\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       7.329952             Absolute error\n",
            "MAE                       2.163278             Absolute error\n",
            "RMSE                      2.707388             Absolute error\n",
            "MAPE                      64.5099%             Percentage error\n",
            "RRMSE                     1.394344             Absolute error\n",
            "RMAE                      0.490678             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         2.15x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            2.3219             bits\n",
            "Symbol_Entropy_Norm       1.0000             (0-1 scale)\n",
            "Quantization_Entropy      6.2960             bits\n",
            "Effective_Alphabet_Size   5.00             symbols\n",
            "2-Gram_Entropy            2.3219             bits\n",
            "3-Gram_Entropy            2.3219             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           7306                 Data points\n",
            "Reconstructed_Length      7306                 Data points\n",
            "Num_Symbols               1135                 Total symbols\n",
            "Unique_Symbols            5                    Distinct symbols\n",
            "Quantized_Shape           (13620, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.4088\n",
            "  Std: 1.9417\n",
            "  Min: 0.6800\n",
            "  Max: 11.9000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.5441\n",
            "  Std: 1.9928\n",
            "  Min: 0.6941\n",
            "  Max: 11.8000\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 2.15x size reduction\n",
            "Reconstruction Accuracy: RMSE = 2.7074\n",
            "Relative Error: MAPE = 64.51%\n",
            "Information Content: Symbol Entropy = 2.32 bits\n",
            "Effective Alphabet Size: 5.0 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: Moderate entropy (structured)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a0_q1\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --no_quant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjOzDXu3Kx8d",
        "outputId": "66c6e892-5560-4f5e-c667-d55e68f89ab6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: TIMESERIES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: timeseries\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Enabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Disabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Disabled\n",
            "================================================================================\n",
            "Loaded time series: 7306 points from data/timeseries.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (13112, 2)\n",
            "  Number of symbol sequences: 227\n",
            "  Total symbols generated: 13112\n",
            "  Unique symbols: 26\n",
            "  Symbol diversity: 0.20%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 7306 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: TIMESERIES p1_a1_q0\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       0.288126             Absolute error\n",
            "MAE                       0.380585             Absolute error\n",
            "RMSE                      0.536774             Absolute error\n",
            "MAPE                      10.4584%             Percentage error\n",
            "RRMSE                     0.276446             Absolute error\n",
            "RMAE                      0.086325             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         4.46x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            4.1049             bits\n",
            "Symbol_Entropy_Norm       0.8733             (0-1 scale)\n",
            "Quantization_Entropy      4.4448             bits\n",
            "Effective_Alphabet_Size   17.21             symbols\n",
            "2-Gram_Entropy            8.1372             bits\n",
            "3-Gram_Entropy            11.4467             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           7306                 Data points\n",
            "Reconstructed_Length      7306                 Data points\n",
            "Num_Symbols               13112                Total symbols\n",
            "Unique_Symbols            26                   Distinct symbols\n",
            "Quantized_Shape           (13112, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.4088\n",
            "  Std: 1.9417\n",
            "  Min: 0.6800\n",
            "  Max: 11.9000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.3974\n",
            "  Std: 2.0406\n",
            "  Min: -0.1377\n",
            "  Max: 12.5494\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 4.46x size reduction\n",
            "Reconstruction Accuracy: RMSE = 0.5368\n",
            "Relative Error: MAPE = 10.46%\n",
            "Information Content: Symbol Entropy = 4.10 bits\n",
            "Effective Alphabet Size: 17.2 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: High entropy (complex patterns)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a1_q0\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset timeseries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci5I_-tLu5v1",
        "outputId": "d7288ba2-e7e4-4641-e8ad-f2468ecf5277",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "--- Dataset: TIMESERIES ---\n",
            "Config: Patch=60, Stride=32, K=26\n",
            "Applying HSQP transformation...\n",
            "Quantized Features Shape: (13112, 2)\n",
            "Reconstructing time series...\n",
            "\n",
            "--- Metrics ---\n",
            "Compression Ratio: 1.49x\n",
            "RMSE: 0.5368\n",
            "MAE:  0.3806\n",
            "Experiment complete. Plots saved with tag: pTrue_aTrue_qTrue\n",
            "Results for timeseries: RMSE = 0.5368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset timeseries --seed 123"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WOqF3g3w9Tt",
        "outputId": "84c61325-4d49-4761-855a-74fffd98003b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 123\n",
            "--- Dataset: TIMESERIES ---\n",
            "Config: Patch=60, Stride=32, K=26\n",
            "Results for timeseries: RMSE = 0.5095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run with entropy analysis\n",
        "!python main.py --dataset electricity --entropy_analysis --save_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTEfjHN6LmiT",
        "outputId": "410773e2-6ae2-40a2-e62f-9134155e584c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: ELECTRICITY\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: electricity\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Enabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Enabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Enabled\n",
            "================================================================================\n",
            "Loaded time series: 26304 points from data/electricity.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (41672, 2)\n",
            "  Number of symbol sequences: 821\n",
            "  Total symbols generated: 41672\n",
            "  Unique symbols: 26\n",
            "  Symbol diversity: 0.06%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 26304 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: ELECTRICITY p1_a1_q1\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       138.689527             Absolute error\n",
            "MAE                       6.752451             Absolute error\n",
            "RMSE                      11.776652             Absolute error\n",
            "MAPE                      inf%             Percentage error\n",
            "RRMSE                     0.488117             Absolute error\n",
            "RMAE                      0.290256             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         1.68x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            3.3969             bits\n",
            "Symbol_Entropy_Norm       0.7227             (0-1 scale)\n",
            "Quantization_Entropy      3.1639             bits\n",
            "Effective_Alphabet_Size   10.53             symbols\n",
            "2-Gram_Entropy            6.3140             bits\n",
            "3-Gram_Entropy            8.8628             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           26304                Data points\n",
            "Reconstructed_Length      26304                Data points\n",
            "Num_Symbols               41672                Total symbols\n",
            "Unique_Symbols            26                   Distinct symbols\n",
            "Quantized_Shape           (41672, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TOKEN ENTROPY ANALYSIS - ELECTRICITY\n",
            "================================================================================\n",
            "\n",
            " SYMBOL ENTROPY:\n",
            "  Shannon Entropy: 3.3969 bits\n",
            "  Normalized Entropy: 0.7227 (0-1 scale)\n",
            "  Max Possible Entropy: 4.7004 bits\n",
            "  Effective Alphabet Size: 10.53\n",
            "  Unique Symbols: 26\n",
            "  Total Symbols: 41672\n",
            "  Symbol Uniqueness Ratio: 0.0006\n",
            "  2-gram Entropy: 6.3140 bits\n",
            "  3-gram Entropy: 8.8628 bits\n",
            "\n",
            " QUANTIZATION ENTROPY:\n",
            "  Quantization Entropy: 3.1639 bits\n",
            "  Quantization Sparsity: 0.6659%\n",
            "  Unique Quantized Values: 173\n",
            "  Value Range: [-128.0000, 127.0000]\n",
            "  Mean ± Std: -4.4071 ± 10.4180\n",
            "\n",
            " JOINT ENTROPY & MUTUAL INFORMATION:\n",
            "  Joint Entropy Upper Bound: 6.5608 bits\n",
            "  Mutual Information (approx): 0.0000 bits\n",
            "  Normalized Mutual Information: 0.0000\n",
            "\n",
            " ENTROPY INTERPRETATION:\n",
            "  Symbol Entropy (3.40 bits): MODERATE - Some structure present\n",
            "================================================================================\n",
            "Metrics saved to: results/metrics_electricity_p1_a1_q1.csv\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 26304\n",
            "  Mean: 23.2638\n",
            "  Std: 24.1267\n",
            "  Min: 0.0000\n",
            "  Max: 140.0000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 26304\n",
            "  Mean: 23.3548\n",
            "  Std: 23.7312\n",
            "  Min: -25.0828\n",
            "  Max: 135.5520\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 1.68x size reduction\n",
            "Reconstruction Accuracy: RMSE = 11.7767\n",
            "Relative Error: MAPE = inf%\n",
            "Information Content: Symbol Entropy = 3.40 bits\n",
            "Effective Alphabet Size: 10.5 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: High entropy (complex patterns)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a1_q1\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ablation study with entropy\n",
        "!python main.py --dataset etth1 --no_quant --entropy_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgNPERvoMMvM",
        "outputId": "aed32a6a-ffa0-485a-cf0f-8ecff7a31108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: ETTH1\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: etth1\n",
            "  Patch Length: 144\n",
            "  Stride: 72\n",
            "  K (symbols): 20\n",
            "  Patching: Enabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Disabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Enabled\n",
            "================================================================================\n",
            "Loaded time series: 17420 points from data/ETTh1.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (33161, 2)\n",
            "  Number of symbol sequences: 240\n",
            "  Total symbols generated: 33161\n",
            "  Unique symbols: 20\n",
            "  Symbol diversity: 0.06%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 17420 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: ETTH1 p1_a1_q0\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       12.591516             Absolute error\n",
            "MAE                       2.456813             Absolute error\n",
            "RMSE                      3.548453             Absolute error\n",
            "MAPE                      9238456686.2175%             Percentage error\n",
            "RRMSE                     0.502077             Absolute error\n",
            "RMAE                      0.262377             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         4.20x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            3.5799             bits\n",
            "Symbol_Entropy_Norm       0.8283             (0-1 scale)\n",
            "Quantization_Entropy      3.4461             bits\n",
            "Effective_Alphabet_Size   11.96             symbols\n",
            "2-Gram_Entropy            6.9701             bits\n",
            "3-Gram_Entropy            10.0975             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           17420                Data points\n",
            "Reconstructed_Length      17420                Data points\n",
            "Num_Symbols               33161                Total symbols\n",
            "Unique_Symbols            20                   Distinct symbols\n",
            "Quantized_Shape           (33161, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TOKEN ENTROPY ANALYSIS - ETTH1\n",
            "================================================================================\n",
            "\n",
            " SYMBOL ENTROPY:\n",
            "  Shannon Entropy: 3.5799 bits\n",
            "  Normalized Entropy: 0.8283 (0-1 scale)\n",
            "  Max Possible Entropy: 4.3219 bits\n",
            "  Effective Alphabet Size: 11.96\n",
            "  Unique Symbols: 20\n",
            "  Total Symbols: 33161\n",
            "  Symbol Uniqueness Ratio: 0.0006\n",
            "  2-gram Entropy: 6.9701 bits\n",
            "  3-gram Entropy: 10.0975 bits\n",
            "\n",
            " QUANTIZATION ENTROPY:\n",
            "  Quantization Entropy: 3.4461 bits\n",
            "  Quantization Sparsity: 0.9243%\n",
            "  Unique Quantized Values: 115\n",
            "  Value Range: [-21.0990, 57.0000]\n",
            "  Mean ± Std: 0.5143 ± 2.2340\n",
            "\n",
            " JOINT ENTROPY & MUTUAL INFORMATION:\n",
            "  Joint Entropy Upper Bound: 7.0259 bits\n",
            "  Mutual Information (approx): 0.0000 bits\n",
            "  Normalized Mutual Information: 0.0000\n",
            "\n",
            " ENTROPY INTERPRETATION:\n",
            "  Symbol Entropy (3.58 bits): MODERATE - Some structure present\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 17420\n",
            "  Mean: 7.3751\n",
            "  Std: 7.0675\n",
            "  Min: -22.7060\n",
            "  Max: 23.6440\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 17420\n",
            "  Mean: 7.4281\n",
            "  Std: 7.2224\n",
            "  Min: -26.6839\n",
            "  Max: 25.2275\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 4.20x size reduction\n",
            "Reconstruction Accuracy: RMSE = 3.5485\n",
            "Relative Error: MAPE = 9238456686.22%\n",
            "Information Content: Symbol Entropy = 3.58 bits\n",
            "Effective Alphabet Size: 12.0 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: High entropy (complex patterns)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a1_q0\n",
            "Output Directory: results\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete analysis with all metrics\n",
        "!python main.py --dataset timeseries --entropy_analysis --save_metrics --output_dir results/full_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50e4KtPkMOu2",
        "outputId": "7a8c7308-d30e-4f74-de2d-b7f046175142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to: 42\n",
            "================================================================================\n",
            "HSQP EXPERIMENT: TIMESERIES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Dataset: timeseries\n",
            "  Patch Length: 60\n",
            "  Stride: 32\n",
            "  K (symbols): 26\n",
            "  Patching: Enabled\n",
            "  ABBA: Enabled\n",
            "  Quantization: Enabled\n",
            "  Seed: 42\n",
            "  Entropy Analysis: Enabled\n",
            "================================================================================\n",
            "Loaded time series: 7306 points from data/timeseries.csv\n",
            "\n",
            "Applying HSQP transformation...\n",
            "✓ Transformation successful\n",
            "  Quantized Features Shape: (13112, 2)\n",
            "  Number of symbol sequences: 227\n",
            "  Total symbols generated: 13112\n",
            "  Unique symbols: 26\n",
            "  Symbol diversity: 0.20%\n",
            "\n",
            "Reconstructing time series...\n",
            "✓ Reconstruction successful: 7306 points\n",
            "\n",
            "================================================================================\n",
            "CALCULATING METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE METRICS REPORT\n",
            "Dataset: TIMESERIES p1_a1_q1\n",
            "================================================================================\n",
            "Metric                    Value                Unit/Description                   \n",
            "--------------------------------------------------------------------------------\n",
            "MSE                       0.288126             Absolute error\n",
            "MAE                       0.380585             Absolute error\n",
            "RMSE                      0.536774             Absolute error\n",
            "MAPE                      10.4584%             Percentage error\n",
            "RRMSE                     0.276446             Absolute error\n",
            "RMAE                      0.086325             Absolute error\n",
            "--------------------------------------------------------------------------------\n",
            "Compression_Ratio         1.49x             Size reduction factor\n",
            "--------------------------------------------------------------------------------\n",
            "TOKEN ENTROPY METRICS    \n",
            "-------------------------\n",
            "Symbol_Entropy            4.1049             bits\n",
            "Symbol_Entropy_Norm       0.8733             (0-1 scale)\n",
            "Quantization_Entropy      4.4427             bits\n",
            "Effective_Alphabet_Size   17.21             symbols\n",
            "2-Gram_Entropy            8.1372             bits\n",
            "3-Gram_Entropy            11.4467             bits\n",
            "--------------------------------------------------------------------------------\n",
            "Original_Length           7306                 Data points\n",
            "Reconstructed_Length      7306                 Data points\n",
            "Num_Symbols               13112                Total symbols\n",
            "Unique_Symbols            26                   Distinct symbols\n",
            "Quantized_Shape           (13112, 2)           (patches × features)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TOKEN ENTROPY ANALYSIS - TIMESERIES\n",
            "================================================================================\n",
            "\n",
            " SYMBOL ENTROPY:\n",
            "  Shannon Entropy: 4.1049 bits\n",
            "  Normalized Entropy: 0.8733 (0-1 scale)\n",
            "  Max Possible Entropy: 4.7004 bits\n",
            "  Effective Alphabet Size: 17.21\n",
            "  Unique Symbols: 26\n",
            "  Total Symbols: 13112\n",
            "  Symbol Uniqueness Ratio: 0.0020\n",
            "  2-gram Entropy: 8.1372 bits\n",
            "  3-gram Entropy: 11.4467 bits\n",
            "\n",
            " QUANTIZATION ENTROPY:\n",
            "  Quantization Entropy: 4.4427 bits\n",
            "  Quantization Sparsity: 0.8046%\n",
            "  Unique Quantized Values: 206\n",
            "  Value Range: [-128.0000, 127.0000]\n",
            "  Mean ± Std: 17.2908 ± 24.1621\n",
            "\n",
            " JOINT ENTROPY & MUTUAL INFORMATION:\n",
            "  Joint Entropy Upper Bound: 8.5476 bits\n",
            "  Mutual Information (approx): 0.0000 bits\n",
            "  Normalized Mutual Information: 0.0000\n",
            "\n",
            " ENTROPY INTERPRETATION:\n",
            "  Symbol Entropy (4.10 bits): HIGH - Complex, information-rich\n",
            "================================================================================\n",
            "Metrics saved to: results/full_analysis/metrics_timeseries_p1_a1_q1.csv\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Original Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.4088\n",
            "  Std: 1.9417\n",
            "  Min: 0.6800\n",
            "  Max: 11.9000\n",
            "\n",
            "Reconstructed Time Series:\n",
            "  Length: 7306\n",
            "  Mean: 4.3974\n",
            "  Std: 2.0406\n",
            "  Min: -0.1377\n",
            "  Max: 12.5494\n",
            "\n",
            "================================================================================\n",
            "KEY PERFORMANCE INDICATORS\n",
            "================================================================================\n",
            "Compression Efficiency: 1.49x size reduction\n",
            "Reconstruction Accuracy: RMSE = 0.5368\n",
            "Relative Error: MAPE = 10.46%\n",
            "Information Content: Symbol Entropy = 4.10 bits\n",
            "Effective Alphabet Size: 17.2 symbols\n",
            "Quality: Poor reconstruction (RMSE ≥ 0.5)\n",
            "Symbol Complexity: High entropy (complex patterns)\n",
            "\n",
            "Generating plots...\n",
            "Figure(1200x400)\n",
            "Figure(1200x800)\n",
            "Plots saved to: results/full_analysis/\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "Ablation Tag: p1_a1_q1\n",
            "Output Directory: results/full_analysis\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBnspnjVM9Dl",
        "outputId": "2e95352e-1b3d-4dc7-cfa1-76e936c6e391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ../HSQP ../drive/MyDrive/Ablation"
      ],
      "metadata": {
        "id": "KN5W2ly6OSWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -av --exclude='*.csv' ../HSQP/ /content/drive/MyDrive/Ablation/"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DeNT06NQqVs",
        "outputId": "ad901250-540e-4fc2-d1b5-8edaf04f476a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "HSQP.zip\n",
            "electricity_reconstruction_error.png\n",
            "electricity_time_series_reconstruction.png\n",
            "main.py\n",
            "reconstruction_(pfalse_atrue_qtrue).png\n",
            "reconstruction_(ptrue_afalse_qtrue).png\n",
            "reconstruction_(ptrue_atrue_qfalse).png\n",
            "reconstruction_(ptrue_atrue_qtrue).png\n",
            "requirements.txt\n",
            "config/\n",
            "config/__init__.py\n",
            "data/\n",
            "data/__init__.py\n",
            "data/dataset.py\n",
            "data/__pycache__/\n",
            "data/__pycache__/__init__.cpython-312.pyc\n",
            "data/__pycache__/dataset.cpython-312.pyc\n",
            "library/\n",
            "library/__init__.py\n",
            "library/feature_quantization.py\n",
            "library/time_series.py\n",
            "library/__pycache__/\n",
            "library/__pycache__/__init__.cpython-312.pyc\n",
            "library/__pycache__/feature_quantization.cpython-312.pyc\n",
            "library/__pycache__/time_series.cpython-312.pyc\n",
            "models/\n",
            "models/__init__.py\n",
            "models/hsqp.py\n",
            "models/__pycache__/\n",
            "models/__pycache__/__init__.cpython-312.pyc\n",
            "models/__pycache__/hsqp.cpython-312.pyc\n",
            "utils/\n",
            "utils/__init__.py\n",
            "utils/hsqp_utils.py\n",
            "utils/__pycache__/\n",
            "utils/__pycache__/__init__.cpython-312.pyc\n",
            "utils/__pycache__/hsqp_utils.cpython-312.pyc\n",
            "\n",
            "sent 52,592,625 bytes  received 599 bytes  105,186,448.00 bytes/sec\n",
            "total size is 52,577,134  speedup is 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r HSQP.zip /content/HSQP/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "afkb__3PNQbA",
        "outputId": "69dd7297-14be-48f1-d00f-c830fd71a5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/HSQP/config/ (stored 0%)\n",
            "  adding: content/HSQP/config/__init__.py (stored 0%)\n",
            "  adding: content/HSQP/data/ (stored 0%)\n",
            "  adding: content/HSQP/data/__init__.py (deflated 23%)\n",
            "  adding: content/HSQP/data/dataset.py (deflated 67%)\n",
            "  adding: content/HSQP/library/ (stored 0%)\n",
            "  adding: content/HSQP/library/time_series.py (deflated 76%)\n",
            "  adding: content/HSQP/library/__init__.py (deflated 39%)\n",
            "  adding: content/HSQP/library/feature_quantization.py (deflated 70%)\n",
            "  adding: content/HSQP/main.py (deflated 64%)\n",
            "  adding: content/HSQP/models/ (stored 0%)\n",
            "  adding: content/HSQP/models/__init__.py (stored 0%)\n",
            "  adding: content/HSQP/models/hsqp.py (deflated 67%)\n",
            "  adding: content/HSQP/requirements.txt (deflated 16%)\n",
            "  adding: content/HSQP/utils/ (stored 0%)\n",
            "  adding: content/HSQP/utils/__init__.py (deflated 13%)\n",
            "  adding: content/HSQP/utils/hsqp_utils.py (deflated 72%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}